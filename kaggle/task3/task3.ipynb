{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":120121,"databundleVersionId":14583263,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\nfrom scipy import stats\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nRANDOM_STATE = 42\nTARGET = 'spending_30d'\nN_FOLDS = 7  # Increased for better generalization\n\n# GPU Configuration (Set to your available GPUs)\nUSE_MULTI_GPU = True  # Enable multi-GPU support\nGPU_IDS = [0, 1]  # Your 2 GPUs\n\n# ==========================================\n# ADVANCED FEATURE ENGINEERING\n# ==========================================\ndef create_advanced_features(df):\n    \"\"\"\n    Comprehensive feature engineering focusing on:\n    - Interaction features\n    - Behavioral patterns\n    - Statistical aggregations\n    - Temporal features\n    \"\"\"\n    df = df.copy()\n    \n    # === Base Preprocessing ===\n    cat_cols = ['primary_game', 'platform', 'vip_status', 'segment']\n    for col in cat_cols:\n        df[col] = df[col].fillna(-1).astype(int)\n    \n    zero_cols = [\n        'is_premium_member', 'guild_membership', 'owns_limited_edition', \n        'tournament_participation', 'friend_count', 'social_interactions', \n        'daily_login_streak', 'historical_spending', 'prev_month_spending',\n        'total_transactions', 'avg_transaction_value', 'purchases_on_discount'\n    ]\n    for col in zero_cols:\n        df[col] = df[col].fillna(0)\n    \n    df['days_since_last_purchase'] = df['days_since_last_purchase'].fillna(9999)\n    \n    # Fill remaining numeric columns\n    filled_cols = cat_cols + zero_cols + ['days_since_last_purchase', 'id', 'player_id', TARGET]\n    num_cols = [c for c in df.columns if c not in filled_cols]\n    for col in num_cols:\n        df[col] = df[col].fillna(df[col].median())\n    \n    # === 1. SPENDING BEHAVIOR FEATURES ===\n    # Historical spending patterns\n    df['has_spent_before'] = (df['historical_spending'] > 0).astype(int)\n    df['spending_momentum'] = df['prev_month_spending'] / (df['historical_spending'] + 1)\n    df['spending_acceleration'] = df['prev_month_spending'] - df['historical_spending'] / 12  # Monthly avg\n    \n    # Transaction patterns\n    df['transactions_per_dollar'] = df['total_transactions'] / (df['historical_spending'] + 1)\n    df['avg_transaction_growth'] = df['avg_transaction_value'] / (df['historical_spending'] / (df['total_transactions'] + 1) + 1)\n    \n    # Purchase recency score\n    df['purchase_recency_score'] = 1 / (df['days_since_last_purchase'] + 1)\n    df['is_recent_buyer'] = (df['days_since_last_purchase'] < 30).astype(int)\n    df['is_dormant'] = (df['days_since_last_purchase'] > 180).astype(int)\n    \n    # Discount behavior\n    df['discount_dependency'] = df['purchases_on_discount'] / (df['total_transactions'] + 1)\n    df['full_price_purchases'] = df['total_transactions'] - df['purchases_on_discount']\n    \n    # === 2. ENGAGEMENT FEATURES ===\n    # Social engagement\n    df['social_engagement_ratio'] = df['social_interactions'] / (df['friend_count'] + 1)\n    df['is_social_player'] = (df['friend_count'] > df['friend_count'].median()).astype(int)\n    \n    # Activity intensity\n    df['activity_score'] = (\n        df['daily_login_streak'] * 0.3 + \n        df['social_interactions'] * 0.3 + \n        df['tournament_participation'] * 0.4\n    )\n    \n    # Commitment indicators\n    df['commitment_score'] = (\n        df['is_premium_member'] * 3 + \n        df['guild_membership'] * 2 + \n        df['owns_limited_edition'] * 2 +\n        (df['daily_login_streak'] > 7).astype(int) * 1\n    )\n    \n    # === 3. VALUE FEATURES ===\n    # Lifetime value indicators\n    df['ltv_score'] = df['historical_spending'] / (df['days_since_last_purchase'] + 1)\n    df['monthly_value'] = df['historical_spending'] / 12\n    df['value_consistency'] = df['avg_transaction_value'] / (df['monthly_value'] + 1)\n    \n    # Spending capacity\n    df['spending_capacity'] = df['avg_transaction_value'] * df['daily_login_streak']\n    df['whale_indicator'] = (df['avg_transaction_value'] > df['avg_transaction_value'].quantile(0.9)).astype(int)\n    \n    # === 4. INTERACTION FEATURES ===\n    # VIP interactions\n    df['vip_spending_ratio'] = df['historical_spending'] * (df['vip_status'] + 1)\n    df['vip_activity'] = df['activity_score'] * (df['vip_status'] + 1)\n    \n    # Premium features interaction\n    df['premium_features'] = (\n        df['is_premium_member'] + \n        df['guild_membership'] + \n        df['owns_limited_edition']\n    )\n    df['premium_spending'] = df['historical_spending'] * df['premium_features']\n    \n    # Segment interactions\n    df['segment_value'] = df['segment'] * df['avg_transaction_value']\n    df['segment_activity'] = df['segment'] * df['daily_login_streak']\n    \n    # === 5. STATISTICAL FEATURES ===\n    # Log transformations for skewed features\n    log_features = [\n        'historical_spending', 'prev_month_spending', 'total_transactions',\n        'avg_transaction_value', 'friend_count', 'social_interactions'\n    ]\n    for col in log_features:\n        if col in df.columns:\n            df[f'{col}_log1p'] = np.log1p(df[col])\n            df[f'{col}_sqrt'] = np.sqrt(df[col])\n    \n    # Binned features for categorical interactions\n    df['spending_tier'] = pd.qcut(df['historical_spending'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    df['activity_tier'] = pd.qcut(df['daily_login_streak'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    df['transaction_tier'] = pd.qcut(df['total_transactions'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    \n    # === 6. RISK/CHURN FEATURES ===\n    df['churn_risk'] = (\n        (df['days_since_last_purchase'] > 90).astype(int) * 3 +\n        (df['daily_login_streak'] < 3).astype(int) * 2 +\n        (df['prev_month_spending'] == 0).astype(int) * 2\n    )\n    \n    # Engagement decay\n    df['engagement_decay'] = df['daily_login_streak'] / (df['days_since_last_purchase'] + 1)\n    \n    return df\n\n# ==========================================\n# OPTIMIZED MODEL PARAMETERS\n# ==========================================\n\n# Stage 1: Classification (Spender vs Non-Spender)\nPARAMS_S1_CAT = {\n    'iterations': 2000,\n    'learning_rate': 0.03,\n    'depth': 8,\n    'l2_leaf_reg': 8.0,\n    'border_count': 254,\n    'bagging_temperature': 0.8,\n    'random_strength': 1.5,\n    'scale_pos_weight': 1.0,  # Will be set dynamically\n    'eval_metric': 'Logloss',\n    'random_state': RANDOM_STATE,\n    'verbose': False,\n    'allow_writing_files': False,\n    'task_type': 'GPU',\n    'devices': '0:1' if USE_MULTI_GPU else '0'  # Use both GPUs\n}\n\nPARAMS_S1_LGB = {\n    'n_estimators': 1500,\n    'learning_rate': 0.02,\n    'num_leaves': 40,\n    'max_depth': 10,\n    'min_child_samples': 25,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_lambda': 3.0,\n    'reg_alpha': 1.5,\n    'scale_pos_weight': 1.0,  # Will be set dynamically\n    'objective': 'binary',\n    'metric': 'auc',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\n# Stage 2: Regression (Amount Prediction)\nPARAMS_S2_CAT = {\n    'iterations': 5000,\n    'learning_rate': 0.008,\n    'depth': 7,\n    'l2_leaf_reg': 3.0,\n    'border_count': 200,\n    'bagging_temperature': 0.6,\n    'random_strength': 1.2,\n    'min_data_in_leaf': 10,\n    'loss_function': 'RMSE',\n    'eval_metric': 'RMSE',\n    'random_state': RANDOM_STATE,\n    'verbose': False,\n    'allow_writing_files': False,\n    'task_type': 'GPU',  # GPU acceleration\n    'devices': '0'\n}\n\nPARAMS_S2_LGB_MAIN = {\n    'n_estimators': 4500,\n    'learning_rate': 0.01,\n    'num_leaves': 64,\n    'max_depth': 10,\n    'min_child_samples': 15,\n    'subsample': 0.85,\n    'colsample_bytree': 0.85,\n    'reg_lambda': 2.0,\n    'reg_alpha': 0.8,\n    'min_split_gain': 0.01,\n    'objective': 'regression',\n    'metric': 'rmse',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\nPARAMS_S2_LGB_DEEP = {\n    'n_estimators': 4000,\n    'learning_rate': 0.006,\n    'num_leaves': 150,\n    'max_depth': 15,\n    'min_child_samples': 8,\n    'subsample': 0.9,\n    'colsample_bytree': 0.8,\n    'reg_lambda': 4.0,\n    'reg_alpha': 0.5,\n    'min_split_gain': 0.005,\n    'objective': 'regression',\n    'metric': 'rmse',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\n# Ensemble weights (tuned based on typical performance)\nW_S1 = {'cat': 0.55, 'lgb': 0.45}\nW_S2 = {'cat': 0.50, 'lgb_main': 0.30, 'lgb_deep': 0.20}\n\n# ==========================================\n# DATA LOADING & PREPROCESSING\n# ==========================================\nprint(\"=\" * 60)\nprint(\"ADVANCED TWO-STAGE PREDICTION PIPELINE (GPU ACCELERATED)\")\nprint(\"=\" * 60)\n\n# Check GPU availability\nprint(\"\\nüîç Checking GPU availability...\")\ntry:\n    import subprocess\n    gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader']).decode()\n    print(f\"‚úì GPU Detected: {gpu_info.strip()}\")\nexcept:\n    print(\"‚ö† GPU not detected. Models will fallback to CPU if GPU unavailable.\")\n\nprint(\"\\n[1/5] Loading data...\")\ntrain = pd.read_csv('/kaggle/input/cpe342-karena/public_dataset/task3/train.csv')\ntest = pd.read_csv('/kaggle/input/cpe342-karena/public_dataset/task3/test.csv')\n\nprint(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n\nprint(\"\\n[2/5] Creating advanced features...\")\ntrain_fe = create_advanced_features(train)\ntest_fe = create_advanced_features(test)\n\n# Feature selection\nexclude_cols = ['id', 'player_id', TARGET, 'days_since_last_purchase']  # days_since excluded as it's often leaky\nfeature_cols = [c for c in train_fe.columns if c not in exclude_cols]\n\n# Identify categorical features\ncat_features = [\n    'primary_game', 'platform', 'vip_status', 'segment',\n    'spending_tier', 'activity_tier', 'transaction_tier'\n]\ncat_features = [c for c in cat_features if c in feature_cols]\n\nprint(f\"Total features: {len(feature_cols)}\")\nprint(f\"Categorical features: {len(cat_features)}\")\n\nX = train_fe[feature_cols].copy()\ny_binary = (train_fe[TARGET] > 0).astype(int)\nX_test = test_fe[feature_cols].copy()\n\n# Convert categoricals\nfor c in cat_features:\n    X[c] = X[c].astype('category')\n    X_test[c] = X_test[c].astype('category')\n\n# ==========================================\n# STAGE 1: CLASSIFICATION\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[3/5] STAGE 1: Spender Classification\")\nprint(\"=\" * 60)\n\n# Calculate class imbalance\npos_rate = y_binary.mean()\nscale_pos_weight = (1 - pos_rate) / pos_rate\nprint(f\"Positive class rate: {pos_rate:.4f}\")\nprint(f\"Scale pos weight: {scale_pos_weight:.4f}\")\n\nPARAMS_S1_CAT['scale_pos_weight'] = scale_pos_weight\nPARAMS_S1_LGB['scale_pos_weight'] = scale_pos_weight\n\nkf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\noof_prob_s1 = np.zeros(len(X))\ntest_prob_s1 = np.zeros(len(X_test))\nfold_aucs = []\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X, y_binary), 1):\n    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y_binary.iloc[tr_idx], y_binary.iloc[val_idx]\n    \n    # CatBoost\n    m1 = CatBoostClassifier(**PARAMS_S1_CAT, cat_features=cat_features)\n    m1.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=100)\n    p1_val = m1.predict_proba(X_val)[:, 1]\n    p1_test = m1.predict_proba(X_test)[:, 1]\n    \n    # LightGBM\n    m2 = lgb.LGBMClassifier(**PARAMS_S1_LGB)\n    m2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n    p2_val = m2.predict_proba(X_val)[:, 1]\n    p2_test = m2.predict_proba(X_test)[:, 1]\n    \n    # Ensemble\n    fold_pred = W_S1['cat'] * p1_val + W_S1['lgb'] * p2_val\n    oof_prob_s1[val_idx] = fold_pred\n    test_prob_s1 += (W_S1['cat'] * p1_test + W_S1['lgb'] * p2_test) / N_FOLDS\n    \n    fold_auc = roc_auc_score(y_val, fold_pred)\n    fold_aucs.append(fold_auc)\n    print(f\"    AUC: {fold_auc:.5f}\")\n\n# Overall metrics\noverall_auc = roc_auc_score(y_binary, oof_prob_s1)\nprint(f\"\\n  Overall OOF AUC: {overall_auc:.5f} ¬± {np.std(fold_aucs):.5f}\")\n\n# Optimize threshold\nbest_acc = 0\nbest_thresh = 0.5\nfor t in np.arange(0.2, 0.8, 0.005):\n    acc = accuracy_score(y_binary, (oof_prob_s1 > t).astype(int))\n    if acc > best_acc:\n        best_acc, best_thresh = acc, t\n\nprint(f\"  Optimal Threshold: {best_thresh:.4f} (Accuracy: {best_acc:.4f})\")\n\n# ==========================================\n# STAGE 2: REGRESSION\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[4/5] STAGE 2: Amount Prediction (Spenders Only)\")\nprint(\"=\" * 60)\n\nmask_spenders = train_fe[TARGET] > 0\nX_reg = train_fe[mask_spenders][feature_cols].reset_index(drop=True)\ny_reg = train_fe[mask_spenders][TARGET].reset_index(drop=True)\ny_reg_log = np.log1p(y_reg)\n\nprint(f\"Training on {len(X_reg)} spenders\")\nprint(f\"Mean spending: ‡∏ø{y_reg.mean():.2f}, Median: ‡∏ø{y_reg.median():.2f}\")\n\n# Convert categoricals\nfor c in cat_features:\n    X_reg[c] = X_reg[c].astype('category')\n\noof_amount_s2 = np.zeros(len(X_reg))\ntest_amount_s2 = np.zeros(len(X_test))\nkf_reg = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfold_rmses = []\n\nfor fold, (tr_idx, val_idx) in enumerate(kf_reg.split(X_reg), 1):\n    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n    X_tr, X_val = X_reg.iloc[tr_idx], X_reg.iloc[val_idx]\n    y_tr, y_val = y_reg_log.iloc[tr_idx], y_reg_log.iloc[val_idx]\n    \n    # CatBoost\n    r1 = CatBoostRegressor(**PARAMS_S2_CAT, cat_features=cat_features)\n    r1.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=150)\n    p1_val = r1.predict(X_val)\n    p1_test = r1.predict(X_test)\n    \n    # LightGBM Main\n    r2 = lgb.LGBMRegressor(**PARAMS_S2_LGB_MAIN)\n    r2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(150, verbose=False)])\n    p2_val = r2.predict(X_val)\n    p2_test = r2.predict(X_test)\n    \n    # LightGBM Deep\n    r3 = lgb.LGBMRegressor(**PARAMS_S2_LGB_DEEP)\n    r3.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(150, verbose=False)])\n    p3_val = r3.predict(X_val)\n    p3_test = r3.predict(X_test)\n    \n    # Ensemble (log space)\n    fold_pred = (\n        W_S2['cat'] * p1_val + \n        W_S2['lgb_main'] * p2_val + \n        W_S2['lgb_deep'] * p3_val\n    )\n    oof_amount_s2[val_idx] = fold_pred\n    test_amount_s2 += (\n        W_S2['cat'] * p1_test + \n        W_S2['lgb_main'] * p2_test + \n        W_S2['lgb_deep'] * p3_test\n    ) / N_FOLDS\n    \n    fold_rmse = np.sqrt(mean_squared_error(y_val, fold_pred))\n    fold_rmses.append(fold_rmse)\n    print(f\"    RMSE (log): {fold_rmse:.5f}\")\n\n# Evaluation\nrmse_log = np.sqrt(mean_squared_error(y_reg_log, oof_amount_s2))\noof_amount_raw = np.expm1(oof_amount_s2)\nrmse_raw = np.sqrt(mean_squared_error(y_reg, oof_amount_raw))\n\nprint(f\"\\n  Overall OOF RMSE (log): {rmse_log:.5f} ¬± {np.std(fold_rmses):.5f}\")\nprint(f\"  Overall OOF RMSE (THB): ‡∏ø{rmse_raw:.2f}\")\n\n# ==========================================\n# FINAL PREDICTIONS\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[5/5] Generating Final Predictions\")\nprint(\"=\" * 60)\n\n# Convert test predictions from log to raw\ntest_amount_raw = np.expm1(test_amount_s2)\n\n# Combine stages with threshold\nfinal_preds = np.where(test_prob_s1 > best_thresh, test_amount_raw, 0)\n\n# Safety clip\nfinal_preds = np.clip(final_preds, 0, 500000)\n\n# Statistics\nprint(f\"\\nPrediction Statistics:\")\nprint(f\"  Threshold: {best_thresh:.4f}\")\nprint(f\"  Predicted spenders: {np.sum(final_preds > 0):,} / {len(final_preds):,} ({100*np.sum(final_preds > 0)/len(final_preds):.2f}%)\")\nprint(f\"  Mean prediction: ‡∏ø{final_preds[final_preds > 0].mean():.2f}\")\nprint(f\"  Median prediction: ‡∏ø{np.median(final_preds[final_preds > 0]):.2f}\")\nprint(f\"  Max prediction: ‡∏ø{final_preds.max():.2f}\")\nprint(f\"  Total predicted revenue: ‡∏ø{final_preds.sum():,.2f}\")\n\n# Save submission\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'spending_30d': final_preds\n})\nsubmission.to_csv('submission_enhanced.csv', index=False)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úì Pipeline Complete! Submission saved as 'submission_enhanced.csv'\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T21:33:12.195361Z","iopub.execute_input":"2025-11-24T21:33:12.195683Z","iopub.status.idle":"2025-11-24T22:09:23.867520Z","shell.execute_reply.started":"2025-11-24T21:33:12.195661Z","shell.execute_reply":"2025-11-24T22:09:23.866825Z"}},"outputs":[{"name":"stdout","text":"============================================================\nADVANCED TWO-STAGE PREDICTION PIPELINE (GPU ACCELERATED)\n============================================================\n\nüîç Checking GPU availability...\n‚úì GPU Detected: Tesla T4, 15360 MiB\nTesla T4, 15360 MiB\n\n[1/5] Loading data...\nTrain shape: (104000, 35), Test shape: (25889, 34)\n\n[2/5] Creating advanced features...\nTotal features: 73\nCategorical features: 7\n\n============================================================\n[3/5] STAGE 1: Spender Classification\n============================================================\nPositive class rate: 0.5180\nScale pos weight: 0.9306\n\n  Fold 1/7\n    AUC: 0.78826\n\n  Fold 2/7\n    AUC: 0.78667\n\n  Fold 3/7\n    AUC: 0.78298\n\n  Fold 4/7\n    AUC: 0.78354\n\n  Fold 5/7\n    AUC: 0.78035\n\n  Fold 6/7\n    AUC: 0.78393\n\n  Fold 7/7\n    AUC: 0.78375\n\n  Overall OOF AUC: 0.78384 ¬± 0.00238\n  Optimal Threshold: 0.4700 (Accuracy: 0.7198)\n\n============================================================\n[4/5] STAGE 2: Amount Prediction (Spenders Only)\n============================================================\nTraining on 53868 spenders\nMean spending: ‡∏ø20019.98, Median: ‡∏ø2297.29\n\n  Fold 1/7\n    RMSE (log): 0.22014\n\n  Fold 2/7\n    RMSE (log): 0.21886\n\n  Fold 3/7\n    RMSE (log): 0.22397\n\n  Fold 4/7\n    RMSE (log): 0.20172\n\n  Fold 5/7\n    RMSE (log): 0.22872\n\n  Fold 6/7\n    RMSE (log): 0.19202\n\n  Fold 7/7\n    RMSE (log): 0.19500\n\n  Overall OOF RMSE (log): 0.21194 ¬± 0.01378\n  Overall OOF RMSE (THB): ‡∏ø14033.87\n\n============================================================\n[5/5] Generating Final Predictions\n============================================================\n\nPrediction Statistics:\n  Threshold: 0.4700\n  Predicted spenders: 13,031 / 25,889 (50.33%)\n  Mean prediction: ‡∏ø22366.32\n  Median prediction: ‡∏ø3145.36\n  Max prediction: ‡∏ø281905.27\n  Total predicted revenue: ‡∏ø291,455,473.33\n\n============================================================\n‚úì Pipeline Complete! Submission saved as 'submission_enhanced.csv'\n============================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\nfrom scipy import stats\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nRANDOM_STATE = 42\nTARGET = 'spending_30d'\nN_FOLDS = 7  # Increased for better generalization\n\n# GPU Configuration (Set to your available GPUs)\nUSE_MULTI_GPU = True  # Enable multi-GPU support\nGPU_IDS = [0, 1]  # Your 2 GPUs\n\n# ==========================================\n# ADVANCED FEATURE ENGINEERING\n# ==========================================\ndef create_advanced_features(df):\n    \"\"\"\n    Comprehensive feature engineering focusing on:\n    - Interaction features\n    - Behavioral patterns\n    - Statistical aggregations\n    - Temporal features\n    \"\"\"\n    df = df.copy()\n    \n    # === Base Preprocessing ===\n    cat_cols = ['primary_game', 'platform', 'vip_status', 'segment']\n    for col in cat_cols:\n        df[col] = df[col].fillna(-1).astype(int)\n    \n    zero_cols = [\n        'is_premium_member', 'guild_membership', 'owns_limited_edition', \n        'tournament_participation', 'friend_count', 'social_interactions', \n        'daily_login_streak', 'historical_spending', 'prev_month_spending',\n        'total_transactions', 'avg_transaction_value', 'purchases_on_discount'\n    ]\n    for col in zero_cols:\n        df[col] = df[col].fillna(0)\n    \n    df['days_since_last_purchase'] = df['days_since_last_purchase'].fillna(9999)\n    \n    # Fill remaining numeric columns\n    filled_cols = cat_cols + zero_cols + ['days_since_last_purchase', 'id', 'player_id', TARGET]\n    num_cols = [c for c in df.columns if c not in filled_cols]\n    for col in num_cols:\n        df[col] = df[col].fillna(df[col].median())\n    \n    # === 1. SPENDING BEHAVIOR FEATURES ===\n    # Historical spending patterns\n    df['has_spent_before'] = (df['historical_spending'] > 0).astype(int)\n    df['spending_momentum'] = df['prev_month_spending'] / (df['historical_spending'] + 1)\n    df['spending_acceleration'] = df['prev_month_spending'] - df['historical_spending'] / 12  # Monthly avg\n    \n    # Transaction patterns\n    df['transactions_per_dollar'] = df['total_transactions'] / (df['historical_spending'] + 1)\n    df['avg_transaction_growth'] = df['avg_transaction_value'] / (df['historical_spending'] / (df['total_transactions'] + 1) + 1)\n    \n    # Purchase recency score\n    df['purchase_recency_score'] = 1 / (df['days_since_last_purchase'] + 1)\n    df['is_recent_buyer'] = (df['days_since_last_purchase'] < 30).astype(int)\n    df['is_dormant'] = (df['days_since_last_purchase'] > 180).astype(int)\n    \n    # Discount behavior\n    df['discount_dependency'] = df['purchases_on_discount'] / (df['total_transactions'] + 1)\n    df['full_price_purchases'] = df['total_transactions'] - df['purchases_on_discount']\n    \n    # === 2. ENGAGEMENT FEATURES ===\n    # Social engagement\n    df['social_engagement_ratio'] = df['social_interactions'] / (df['friend_count'] + 1)\n    df['is_social_player'] = (df['friend_count'] > df['friend_count'].median()).astype(int)\n    \n    # Activity intensity\n    df['activity_score'] = (\n        df['daily_login_streak'] * 0.3 + \n        df['social_interactions'] * 0.3 + \n        df['tournament_participation'] * 0.4\n    )\n    \n    # Commitment indicators\n    df['commitment_score'] = (\n        df['is_premium_member'] * 3 + \n        df['guild_membership'] * 2 + \n        df['owns_limited_edition'] * 2 +\n        (df['daily_login_streak'] > 7).astype(int) * 1\n    )\n    \n    # === 3. VALUE FEATURES ===\n    # Lifetime value indicators\n    df['ltv_score'] = df['historical_spending'] / (df['days_since_last_purchase'] + 1)\n    df['monthly_value'] = df['historical_spending'] / 12\n    df['value_consistency'] = df['avg_transaction_value'] / (df['monthly_value'] + 1)\n    \n    # Spending capacity\n    df['spending_capacity'] = df['avg_transaction_value'] * df['daily_login_streak']\n    df['whale_indicator'] = (df['avg_transaction_value'] > df['avg_transaction_value'].quantile(0.9)).astype(int)\n    \n    # === 4. INTERACTION FEATURES ===\n    # VIP interactions\n    df['vip_spending_ratio'] = df['historical_spending'] * (df['vip_status'] + 1)\n    df['vip_activity'] = df['activity_score'] * (df['vip_status'] + 1)\n    \n    # Premium features interaction\n    df['premium_features'] = (\n        df['is_premium_member'] + \n        df['guild_membership'] + \n        df['owns_limited_edition']\n    )\n    df['premium_spending'] = df['historical_spending'] * df['premium_features']\n    \n    # Segment interactions\n    df['segment_value'] = df['segment'] * df['avg_transaction_value']\n    df['segment_activity'] = df['segment'] * df['daily_login_streak']\n    \n    # === 5. STATISTICAL FEATURES ===\n    # Log transformations for skewed features\n    log_features = [\n        'historical_spending', 'prev_month_spending', 'total_transactions',\n        'avg_transaction_value', 'friend_count', 'social_interactions'\n    ]\n    for col in log_features:\n        if col in df.columns:\n            df[f'{col}_log1p'] = np.log1p(df[col])\n            df[f'{col}_sqrt'] = np.sqrt(df[col])\n    \n    # Binned features for categorical interactions\n    df['spending_tier'] = pd.qcut(df['historical_spending'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    df['activity_tier'] = pd.qcut(df['daily_login_streak'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    df['transaction_tier'] = pd.qcut(df['total_transactions'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    \n    # === 6. RISK/CHURN FEATURES ===\n    df['churn_risk'] = (\n        (df['days_since_last_purchase'] > 90).astype(int) * 3 +\n        (df['daily_login_streak'] < 3).astype(int) * 2 +\n        (df['prev_month_spending'] == 0).astype(int) * 2\n    )\n    \n    # Engagement decay\n    df['engagement_decay'] = df['daily_login_streak'] / (df['days_since_last_purchase'] + 1)\n    \n    return df\n\n# ==========================================\n# OPTIMIZED MODEL PARAMETERS\n# ==========================================\n\n# Stage 1: Classification (Spender vs Non-Spender)\nPARAMS_S1_CAT = {\n    'iterations': 2000,\n    'learning_rate': 0.03,\n    'depth': 8,\n    'l2_leaf_reg': 8.0,\n    'border_count': 254,\n    'bagging_temperature': 0.8,\n    'random_strength': 1.5,\n    'scale_pos_weight': 1.0,  # Will be set dynamically\n    'eval_metric': 'Logloss',  # GPU-compatible metric (AUC not supported on GPU)\n    'random_state': RANDOM_STATE,\n    'verbose': False,\n    'allow_writing_files': False,\n    'task_type': 'GPU',\n    'devices': '0:1' if USE_MULTI_GPU else '0'  # Use both GPUs\n}\n\nPARAMS_S1_LGB = {\n    'n_estimators': 1500,\n    'learning_rate': 0.02,\n    'num_leaves': 40,\n    'max_depth': 10,\n    'min_child_samples': 25,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_lambda': 3.0,\n    'reg_alpha': 1.5,\n    'scale_pos_weight': 1.0,  # Will be set dynamically\n    'objective': 'binary',\n    'metric': 'auc',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\n# Stage 2: Regression (Amount Prediction)\nPARAMS_S2_CAT = {\n    'iterations': 5000,\n    'learning_rate': 0.007,  # Slightly lower for more stable learning\n    'depth': 6,  # Reduced from 7 to prevent overfitting\n    'l2_leaf_reg': 5.0,  # Increased regularization\n    'border_count': 200,\n    'bagging_temperature': 0.7,  # Increased for more randomness\n    'random_strength': 1.5,  # Increased for better generalization\n    'min_data_in_leaf': 15,  # Increased from 10\n    'loss_function': 'RMSE',\n    'eval_metric': 'RMSE',\n    'random_state': RANDOM_STATE,\n    'verbose': False,\n    'allow_writing_files': False,\n    'task_type': 'GPU',\n    'devices': '0:1' if USE_MULTI_GPU else '0'  # Use both GPUs\n}\n\nPARAMS_S2_LGB_MAIN = {\n    'n_estimators': 4500,\n    'learning_rate': 0.009,  # Slightly lower\n    'num_leaves': 50,  # Reduced from 64\n    'max_depth': 9,  # Reduced from 10\n    'min_child_samples': 20,  # Increased from 15\n    'subsample': 0.82,  # Slightly reduced\n    'colsample_bytree': 0.82,  # Slightly reduced\n    'reg_lambda': 3.0,  # Increased regularization\n    'reg_alpha': 1.2,  # Increased regularization\n    'min_split_gain': 0.015,  # Increased\n    'objective': 'regression',\n    'metric': 'rmse',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\nPARAMS_S2_LGB_DEEP = {\n    'n_estimators': 4000,\n    'learning_rate': 0.006,\n    'num_leaves': 150,\n    'max_depth': 15,\n    'min_child_samples': 8,\n    'subsample': 0.9,\n    'colsample_bytree': 0.8,\n    'reg_lambda': 4.0,\n    'reg_alpha': 0.5,\n    'min_split_gain': 0.005,\n    'objective': 'regression',\n    'metric': 'rmse',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\n# Ensemble weights (tuned based on typical performance)\nW_S1 = {'cat': 0.55, 'lgb': 0.45}\nW_S2 = {'cat': 0.50, 'lgb_main': 0.30, 'lgb_deep': 0.20}\n\n# ==========================================\n# DATA LOADING & PREPROCESSING\n# ==========================================\nprint(\"=\" * 60)\nprint(\"ADVANCED TWO-STAGE PREDICTION PIPELINE (MULTI-GPU)\")\nprint(\"=\" * 60)\n\n# Check GPU availability\nprint(\"\\nüîç Checking GPU availability...\")\ntry:\n    import subprocess\n    gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=index,name,memory.total', '--format=csv,noheader']).decode()\n    gpu_lines = gpu_info.strip().split('\\n')\n    print(f\"‚úì Detected {len(gpu_lines)} GPU(s):\")\n    for line in gpu_lines:\n        print(f\"  - GPU {line}\")\n    \n    if USE_MULTI_GPU and len(gpu_lines) >= 2:\n        print(f\"\\nüöÄ Multi-GPU Mode ENABLED: Using GPU {GPU_IDS}\")\n    else:\n        print(f\"\\n‚ö° Single-GPU Mode: Using GPU {GPU_IDS[0]}\")\nexcept:\n    print(\"‚ö† GPU not detected. Models will fallback to CPU if GPU unavailable.\")\n\nprint(\"\\n[1/5] Loading data...\")\ntrain = pd.read_csv('/kaggle/input/cpe342-karena/public_dataset/task3/train.csv')\ntest = pd.read_csv('/kaggle/input/cpe342-karena/public_dataset/task3/test.csv')\n\nprint(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n\nprint(\"\\n[2/5] Creating advanced features...\")\ntrain_fe = create_advanced_features(train)\ntest_fe = create_advanced_features(test)\n\n# Feature selection\nexclude_cols = ['id', 'player_id', TARGET, 'days_since_last_purchase']  # days_since excluded as it's often leaky\nfeature_cols = [c for c in train_fe.columns if c not in exclude_cols]\n\n# Identify categorical features\ncat_features = [\n    'primary_game', 'platform', 'vip_status', 'segment',\n    'spending_tier', 'activity_tier', 'transaction_tier'\n]\ncat_features = [c for c in cat_features if c in feature_cols]\n\nprint(f\"Total features: {len(feature_cols)}\")\nprint(f\"Categorical features: {len(cat_features)}\")\n\nX = train_fe[feature_cols].copy()\ny_binary = (train_fe[TARGET] > 0).astype(int)\nX_test = test_fe[feature_cols].copy()\n\n# Convert categoricals\nfor c in cat_features:\n    X[c] = X[c].astype('category')\n    X_test[c] = X_test[c].astype('category')\n\n# ==========================================\n# STAGE 1: CLASSIFICATION\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[3/5] STAGE 1: Spender Classification\")\nprint(\"=\" * 60)\n\n# Calculate class imbalance\npos_rate = y_binary.mean()\nscale_pos_weight = (1 - pos_rate) / pos_rate\nprint(f\"Positive class rate: {pos_rate:.4f}\")\nprint(f\"Scale pos weight: {scale_pos_weight:.4f}\")\n\nPARAMS_S1_CAT['scale_pos_weight'] = scale_pos_weight\nPARAMS_S1_LGB['scale_pos_weight'] = scale_pos_weight\n\nkf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\noof_prob_s1 = np.zeros(len(X))\ntest_prob_s1 = np.zeros(len(X_test))\nfold_aucs = []\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X, y_binary), 1):\n    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y_binary.iloc[tr_idx], y_binary.iloc[val_idx]\n    \n    # Dynamically assign GPU for LightGBM models (alternate between GPUs)\n    current_gpu = GPU_IDS[fold % len(GPU_IDS)] if USE_MULTI_GPU else GPU_IDS[0]\n    \n    # CatBoost (uses both GPUs automatically with '0:1')\n    m1 = CatBoostClassifier(**PARAMS_S1_CAT, cat_features=cat_features)\n    m1.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=100)\n    p1_val = m1.predict_proba(X_val)[:, 1]\n    p1_test = m1.predict_proba(X_test)[:, 1]\n    \n    # LightGBM (assign specific GPU per fold for load balancing)\n    params_lgb_fold = PARAMS_S1_LGB.copy()\n    params_lgb_fold['gpu_device_id'] = current_gpu\n    m2 = lgb.LGBMClassifier(**params_lgb_fold)\n    m2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n    p2_val = m2.predict_proba(X_val)[:, 1]\n    p2_test = m2.predict_proba(X_test)[:, 1]\n    \n    # Ensemble\n    fold_pred = W_S1['cat'] * p1_val + W_S1['lgb'] * p2_val\n    oof_prob_s1[val_idx] = fold_pred\n    test_prob_s1 += (W_S1['cat'] * p1_test + W_S1['lgb'] * p2_test) / N_FOLDS\n    \n    fold_auc = roc_auc_score(y_val, fold_pred)\n    fold_aucs.append(fold_auc)\n    print(f\"    AUC: {fold_auc:.5f} [GPU {current_gpu if 'LGB' in str(type(m2)) else '0:1'}]\")\n\n# Overall metrics\noverall_auc = roc_auc_score(y_binary, oof_prob_s1)\nprint(f\"\\n  Overall OOF AUC: {overall_auc:.5f} ¬± {np.std(fold_aucs):.5f}\")\n\n# Optimize threshold\nbest_acc = 0\nbest_thresh = 0.5\nfor t in np.arange(0.2, 0.8, 0.005):\n    acc = accuracy_score(y_binary, (oof_prob_s1 > t).astype(int))\n    if acc > best_acc:\n        best_acc, best_thresh = acc, t\n\nprint(f\"  Optimal Threshold: {best_thresh:.4f} (Accuracy: {best_acc:.4f})\")\n\n# ==========================================\n# STAGE 2: REGRESSION\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[4/5] STAGE 2: Amount Prediction (Spenders Only)\")\nprint(\"=\" * 60)\n\nmask_spenders = train_fe[TARGET] > 0\nX_reg = train_fe[mask_spenders][feature_cols].reset_index(drop=True)\ny_reg = train_fe[mask_spenders][TARGET].reset_index(drop=True)\ny_reg_log = np.log1p(y_reg)\n\nprint(f\"Training on {len(X_reg)} spenders\")\nprint(f\"Mean spending: ‡∏ø{y_reg.mean():.2f}, Median: ‡∏ø{y_reg.median():.2f}\")\n\n# Convert categoricals\nfor c in cat_features:\n    X_reg[c] = X_reg[c].astype('category')\n\noof_amount_s2 = np.zeros(len(X_reg))\ntest_amount_s2 = np.zeros(len(X_test))\nkf_reg = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfold_rmses = []\n\nfor fold, (tr_idx, val_idx) in enumerate(kf_reg.split(X_reg), 1):\n    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n    X_tr, X_val = X_reg.iloc[tr_idx], X_reg.iloc[val_idx]\n    y_tr, y_val = y_reg_log.iloc[tr_idx], y_reg_log.iloc[val_idx]\n    \n    # Dynamically assign GPU for LightGBM models\n    current_gpu = GPU_IDS[fold % len(GPU_IDS)] if USE_MULTI_GPU else GPU_IDS[0]\n    \n    # CatBoost (uses both GPUs)\n    r1 = CatBoostRegressor(**PARAMS_S2_CAT, cat_features=cat_features)\n    r1.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=150)\n    p1_val = r1.predict(X_val)\n    p1_test = r1.predict(X_test)\n    \n    # LightGBM Main (GPU rotation)\n    params_lgb_main_fold = PARAMS_S2_LGB_MAIN.copy()\n    params_lgb_main_fold['gpu_device_id'] = current_gpu\n    r2 = lgb.LGBMRegressor(**params_lgb_main_fold)\n    r2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(150, verbose=False)])\n    p2_val = r2.predict(X_val)\n    p2_test = r2.predict(X_test)\n    \n    # LightGBM Deep (alternate GPU for parallel processing)\n    alternate_gpu = GPU_IDS[(fold + 1) % len(GPU_IDS)] if USE_MULTI_GPU else GPU_IDS[0]\n    params_lgb_deep_fold = PARAMS_S2_LGB_DEEP.copy()\n    params_lgb_deep_fold['gpu_device_id'] = alternate_gpu\n    r3 = lgb.LGBMRegressor(**params_lgb_deep_fold)\n    r3.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(150, verbose=False)])\n    p3_val = r3.predict(X_val)\n    p3_test = r3.predict(X_test)\n    \n    # Ensemble (log space)\n    fold_pred = (\n        W_S2['cat'] * p1_val + \n        W_S2['lgb_main'] * p2_val + \n        W_S2['lgb_deep'] * p3_val\n    )\n    oof_amount_s2[val_idx] = fold_pred\n    test_amount_s2 += (\n        W_S2['cat'] * p1_test + \n        W_S2['lgb_main'] * p2_test + \n        W_S2['lgb_deep'] * p3_test\n    ) / N_FOLDS\n    \n    fold_rmse = np.sqrt(mean_squared_error(y_val, fold_pred))\n    fold_rmses.append(fold_rmse)\n    print(f\"    RMSE (log): {fold_rmse:.5f} [GPU {current_gpu}/{alternate_gpu}]\")\n\n# Evaluation\nrmse_log = np.sqrt(mean_squared_error(y_reg_log, oof_amount_s2))\noof_amount_raw = np.expm1(oof_amount_s2)\nrmse_raw = np.sqrt(mean_squared_error(y_reg, oof_amount_raw))\n\nprint(f\"\\n  Overall OOF RMSE (log): {rmse_log:.5f} ¬± {np.std(fold_rmses):.5f}\")\nprint(f\"  Overall OOF RMSE (THB): ‡∏ø{rmse_raw:.2f}\")\n\n# ==========================================\n# FINAL PREDICTIONS\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[5/5] Generating Final Predictions\")\nprint(\"=\" * 60)\n\n# Convert test predictions from log to raw\ntest_amount_raw = np.expm1(test_amount_s2)\n\n# Combine stages with threshold\nfinal_preds = np.where(test_prob_s1 > best_thresh, test_amount_raw, 0)\n\n# Safety clip\nfinal_preds = np.clip(final_preds, 0, 500000)\n\n# Statistics\nprint(f\"\\nPrediction Statistics:\")\nprint(f\"  Threshold: {best_thresh:.4f}\")\nprint(f\"  Predicted spenders: {np.sum(final_preds > 0):,} / {len(final_preds):,} ({100*np.sum(final_preds > 0)/len(final_preds):.2f}%)\")\nprint(f\"  Mean prediction: ‡∏ø{final_preds[final_preds > 0].mean():.2f}\")\nprint(f\"  Median prediction: ‡∏ø{np.median(final_preds[final_preds > 0]):.2f}\")\nprint(f\"  Max prediction: ‡∏ø{final_preds.max():.2f}\")\nprint(f\"  Total predicted revenue: ‡∏ø{final_preds.sum():,.2f}\")\n\n# Save submission\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'spending_30d': final_preds\n})\nsubmission.to_csv('submission_enhanced.csv', index=False)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úì Pipeline Complete! Submission saved as 'submission_enhanced.csv'\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T22:17:35.718917Z","iopub.execute_input":"2025-11-24T22:17:35.719563Z","iopub.status.idle":"2025-11-24T22:32:26.185548Z","shell.execute_reply.started":"2025-11-24T22:17:35.719538Z","shell.execute_reply":"2025-11-24T22:32:26.184047Z"}},"outputs":[{"name":"stdout","text":"============================================================\nADVANCED TWO-STAGE PREDICTION PIPELINE (MULTI-GPU)\n============================================================\n\nüîç Checking GPU availability...\n‚úì Detected 2 GPU(s):\n  - GPU 0, Tesla T4, 15360 MiB\n  - GPU 1, Tesla T4, 15360 MiB\n\nüöÄ Multi-GPU Mode ENABLED: Using GPU [0, 1]\n\n[1/5] Loading data...\nTrain shape: (104000, 35), Test shape: (25889, 34)\n\n[2/5] Creating advanced features...\nTotal features: 73\nCategorical features: 7\n\n============================================================\n[3/5] STAGE 1: Spender Classification\n============================================================\nPositive class rate: 0.5180\nScale pos weight: 0.9306\n\n  Fold 1/7\n    AUC: 0.78825 [GPU 1]\n\n  Fold 2/7\n    AUC: 0.78668 [GPU 0]\n\n  Fold 3/7\n    AUC: 0.78297 [GPU 1]\n\n  Fold 4/7\n    AUC: 0.78354 [GPU 0]\n\n  Fold 5/7\n    AUC: 0.78035 [GPU 1]\n\n  Fold 6/7\n    AUC: 0.78393 [GPU 0]\n\n  Fold 7/7\n    AUC: 0.78375 [GPU 1]\n\n  Overall OOF AUC: 0.78386 ¬± 0.00238\n  Optimal Threshold: 0.4700 (Accuracy: 0.7198)\n\n============================================================\n[4/5] STAGE 2: Amount Prediction (Spenders Only)\n============================================================\nTraining on 53868 spenders\nMean spending: ‡∏ø20019.98, Median: ‡∏ø2297.29\n\n  Fold 1/7\n    RMSE (log): 0.22103 [GPU 1/0]\n\n  Fold 2/7\n    RMSE (log): 0.21953 [GPU 0/1]\n\n  Fold 3/7\n    RMSE (log): 0.22477 [GPU 1/0]\n\n  Fold 4/7\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/4014001873.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;31m# CatBoost (uses both GPUs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mPARAMS_S2_CAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0mp1_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mp1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss_function'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5874\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\nfrom scipy import stats\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nRANDOM_STATE = 42\nTARGET = 'spending_30d'\nN_FOLDS = 7  # Increased for better generalization\n\n# GPU Configuration (Set to your available GPUs)\nUSE_MULTI_GPU = True  # Enable multi-GPU support\nGPU_IDS = [0, 1]  # Your 2 GPUs\n\n# ==========================================\n# ADVANCED FEATURE ENGINEERING\n# ==========================================\ndef create_advanced_features(df):\n    \"\"\"\n    Comprehensive feature engineering focusing on:\n    - Interaction features\n    - Behavioral patterns\n    - Statistical aggregations\n    - Temporal features\n    \"\"\"\n    df = df.copy()\n    \n    # === Base Preprocessing ===\n    cat_cols = ['primary_game', 'platform', 'vip_status', 'segment']\n    for col in cat_cols:\n        df[col] = df[col].fillna(-1).astype(int)\n    \n    zero_cols = [\n        'is_premium_member', 'guild_membership', 'owns_limited_edition', \n        'tournament_participation', 'friend_count', 'social_interactions', \n        'daily_login_streak', 'historical_spending', 'prev_month_spending',\n        'total_transactions', 'avg_transaction_value', 'purchases_on_discount'\n    ]\n    for col in zero_cols:\n        df[col] = df[col].fillna(0)\n    \n    df['days_since_last_purchase'] = df['days_since_last_purchase'].fillna(9999)\n    \n    # Fill remaining numeric columns\n    filled_cols = cat_cols + zero_cols + ['days_since_last_purchase', 'id', 'player_id', TARGET]\n    num_cols = [c for c in df.columns if c not in filled_cols]\n    for col in num_cols:\n        df[col] = df[col].fillna(df[col].median())\n    \n    # === 1. SPENDING BEHAVIOR FEATURES ===\n    # Historical spending patterns\n    df['has_spent_before'] = (df['historical_spending'] > 0).astype(int)\n    df['spending_momentum'] = df['prev_month_spending'] / (df['historical_spending'] + 1)\n    df['spending_acceleration'] = df['prev_month_spending'] - df['historical_spending'] / 12  # Monthly avg\n    \n    # Transaction patterns\n    df['transactions_per_dollar'] = df['total_transactions'] / (df['historical_spending'] + 1)\n    df['avg_transaction_growth'] = df['avg_transaction_value'] / (df['historical_spending'] / (df['total_transactions'] + 1) + 1)\n    \n    # Purchase recency score\n    df['purchase_recency_score'] = 1 / (df['days_since_last_purchase'] + 1)\n    df['is_recent_buyer'] = (df['days_since_last_purchase'] < 30).astype(int)\n    df['is_dormant'] = (df['days_since_last_purchase'] > 180).astype(int)\n    \n    # Discount behavior\n    df['discount_dependency'] = df['purchases_on_discount'] / (df['total_transactions'] + 1)\n    df['full_price_purchases'] = df['total_transactions'] - df['purchases_on_discount']\n    \n    # === 2. ENGAGEMENT FEATURES ===\n    # Social engagement\n    df['social_engagement_ratio'] = df['social_interactions'] / (df['friend_count'] + 1)\n    df['is_social_player'] = (df['friend_count'] > df['friend_count'].median()).astype(int)\n    \n    # Activity intensity\n    df['activity_score'] = (\n        df['daily_login_streak'] * 0.3 + \n        df['social_interactions'] * 0.3 + \n        df['tournament_participation'] * 0.4\n    )\n    \n    # Commitment indicators\n    df['commitment_score'] = (\n        df['is_premium_member'] * 3 + \n        df['guild_membership'] * 2 + \n        df['owns_limited_edition'] * 2 +\n        (df['daily_login_streak'] > 7).astype(int) * 1\n    )\n    \n    # === 3. VALUE FEATURES ===\n    # Lifetime value indicators\n    df['ltv_score'] = df['historical_spending'] / (df['days_since_last_purchase'] + 1)\n    df['monthly_value'] = df['historical_spending'] / 12\n    df['value_consistency'] = df['avg_transaction_value'] / (df['monthly_value'] + 1)\n    \n    # Spending capacity\n    df['spending_capacity'] = df['avg_transaction_value'] * df['daily_login_streak']\n    df['whale_indicator'] = (df['avg_transaction_value'] > df['avg_transaction_value'].quantile(0.9)).astype(int)\n    \n    # === 4. INTERACTION FEATURES ===\n    # VIP interactions\n    df['vip_spending_ratio'] = df['historical_spending'] * (df['vip_status'] + 1)\n    df['vip_activity'] = df['activity_score'] * (df['vip_status'] + 1)\n    \n    # Premium features interaction\n    df['premium_features'] = (\n        df['is_premium_member'] + \n        df['guild_membership'] + \n        df['owns_limited_edition']\n    )\n    df['premium_spending'] = df['historical_spending'] * df['premium_features']\n    \n    # Segment interactions\n    df['segment_value'] = df['segment'] * df['avg_transaction_value']\n    df['segment_activity'] = df['segment'] * df['daily_login_streak']\n    \n    # === 5. STATISTICAL FEATURES ===\n    # Log transformations for skewed features\n    log_features = [\n        'historical_spending', 'prev_month_spending', 'total_transactions',\n        'avg_transaction_value', 'friend_count', 'social_interactions'\n    ]\n    for col in log_features:\n        if col in df.columns:\n            df[f'{col}_log1p'] = np.log1p(df[col])\n            df[f'{col}_sqrt'] = np.sqrt(df[col])\n    \n    # Binned features for categorical interactions\n    df['spending_tier'] = pd.qcut(df['historical_spending'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    df['activity_tier'] = pd.qcut(df['daily_login_streak'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    df['transaction_tier'] = pd.qcut(df['total_transactions'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n    \n    # === 6. RISK/CHURN FEATURES ===\n    df['churn_risk'] = (\n        (df['days_since_last_purchase'] > 90).astype(int) * 3 +\n        (df['daily_login_streak'] < 3).astype(int) * 2 +\n        (df['prev_month_spending'] == 0).astype(int) * 2\n    )\n    \n    # Engagement decay\n    df['engagement_decay'] = df['daily_login_streak'] / (df['days_since_last_purchase'] + 1)\n    \n    return df\n\n# ==========================================\n# OPTIMIZED MODEL PARAMETERS\n# ==========================================\n\n# Stage 1: Classification (Spender vs Non-Spender)\nPARAMS_S1_CAT = {\n    'iterations': 2000,\n    'learning_rate': 0.03,\n    'depth': 8,\n    'l2_leaf_reg': 8.0,\n    'border_count': 254,\n    'bagging_temperature': 0.8,\n    'random_strength': 1.5,\n    'scale_pos_weight': 1.0,  # Will be set dynamically\n    'eval_metric': 'Logloss',\n    'random_state': RANDOM_STATE,\n    'verbose': False,\n    'allow_writing_files': False,\n    'task_type': 'GPU',\n    'devices': '0:1' if USE_MULTI_GPU else '0'  # Use both GPUs\n}\n\nPARAMS_S1_LGB = {\n    'n_estimators': 1500,\n    'learning_rate': 0.02,\n    'num_leaves': 40,\n    'max_depth': 10,\n    'min_child_samples': 25,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_lambda': 3.0,\n    'reg_alpha': 1.5,\n    'scale_pos_weight': 1.0,  # Will be set dynamically\n    'objective': 'binary',\n    'metric': 'auc',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\n# Stage 2: Regression (Amount Prediction)\nPARAMS_S2_CAT = {\n    'iterations': 5000,\n    'learning_rate': 0.008,\n    'depth': 7,\n    'l2_leaf_reg': 3.0,\n    'border_count': 200,\n    'bagging_temperature': 0.6,\n    'random_strength': 1.2,\n    'min_data_in_leaf': 10,\n    'loss_function': 'RMSE',\n    'eval_metric': 'RMSE',\n    'random_state': RANDOM_STATE,\n    'verbose': False,\n    'allow_writing_files': False,\n    'task_type': 'GPU',  # GPU acceleration\n    'devices': '0'\n}\n\nPARAMS_S2_LGB_MAIN = {\n    'n_estimators': 4500,\n    'learning_rate': 0.01,\n    'num_leaves': 64,\n    'max_depth': 10,\n    'min_child_samples': 15,\n    'subsample': 0.85,\n    'colsample_bytree': 0.85,\n    'reg_lambda': 2.0,\n    'reg_alpha': 0.8,\n    'min_split_gain': 0.01,\n    'objective': 'regression',\n    'metric': 'rmse',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\nPARAMS_S2_LGB_DEEP = {\n    'n_estimators': 4000,\n    'learning_rate': 0.006,\n    'num_leaves': 150,\n    'max_depth': 15,\n    'min_child_samples': 8,\n    'subsample': 0.9,\n    'colsample_bytree': 0.8,\n    'reg_lambda': 4.0,\n    'reg_alpha': 0.5,\n    'min_split_gain': 0.005,\n    'objective': 'regression',\n    'metric': 'rmse',\n    'random_state': RANDOM_STATE,\n    'verbosity': -1,\n    'device': 'gpu',  # GPU acceleration\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n}\n\n# Ensemble weights (tuned based on typical performance)\nW_S1 = {'cat': 0.55, 'lgb': 0.45}\nW_S2 = {'cat': 0.50, 'lgb_main': 0.30, 'lgb_deep': 0.20}\n\n# ==========================================\n# DATA LOADING & PREPROCESSING\n# ==========================================\nprint(\"=\" * 60)\nprint(\"ADVANCED TWO-STAGE PREDICTION PIPELINE (GPU ACCELERATED)\")\nprint(\"=\" * 60)\n\n# Check GPU availability\nprint(\"\\nüîç Checking GPU availability...\")\ntry:\n    import subprocess\n    gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader']).decode()\n    print(f\"‚úì GPU Detected: {gpu_info.strip()}\")\nexcept:\n    print(\"‚ö† GPU not detected. Models will fallback to CPU if GPU unavailable.\")\n\nprint(\"\\n[1/5] Loading data...\")\ntrain = pd.read_csv('/kaggle/input/cpe342-karena/public_dataset/task3/train.csv')\ntest = pd.read_csv('/kaggle/input/cpe342-karena/public_dataset/task3/test.csv')\n\nprint(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n\nprint(\"\\n[2/5] Creating advanced features...\")\ntrain_fe = create_advanced_features(train)\ntest_fe = create_advanced_features(test)\n\n# Feature selection\nexclude_cols = ['id', 'player_id', TARGET, 'days_since_last_purchase']  # days_since excluded as it's often leaky\nfeature_cols = [c for c in train_fe.columns if c not in exclude_cols]\n\n# Identify categorical features\ncat_features = [\n    'primary_game', 'platform', 'vip_status', 'segment',\n    'spending_tier', 'activity_tier', 'transaction_tier'\n]\ncat_features = [c for c in cat_features if c in feature_cols]\n\nprint(f\"Total features: {len(feature_cols)}\")\nprint(f\"Categorical features: {len(cat_features)}\")\n\nX = train_fe[feature_cols].copy()\ny_binary = (train_fe[TARGET] > 0).astype(int)\nX_test = test_fe[feature_cols].copy()\n\n# Convert categoricals\nfor c in cat_features:\n    X[c] = X[c].astype('category')\n    X_test[c] = X_test[c].astype('category')\n\n# ==========================================\n# STAGE 1: CLASSIFICATION\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[3/5] STAGE 1: Spender Classification\")\nprint(\"=\" * 60)\n\n# Calculate class imbalance\npos_rate = y_binary.mean()\nscale_pos_weight = (1 - pos_rate) / pos_rate\nprint(f\"Positive class rate: {pos_rate:.4f}\")\nprint(f\"Scale pos weight: {scale_pos_weight:.4f}\")\n\nPARAMS_S1_CAT['scale_pos_weight'] = scale_pos_weight\nPARAMS_S1_LGB['scale_pos_weight'] = scale_pos_weight\n\nkf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\noof_prob_s1 = np.zeros(len(X))\ntest_prob_s1 = np.zeros(len(X_test))\nfold_aucs = []\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X, y_binary), 1):\n    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y_binary.iloc[tr_idx], y_binary.iloc[val_idx]\n    \n    # CatBoost\n    m1 = CatBoostClassifier(**PARAMS_S1_CAT, cat_features=cat_features)\n    m1.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=100)\n    p1_val = m1.predict_proba(X_val)[:, 1]\n    p1_test = m1.predict_proba(X_test)[:, 1]\n    \n    # LightGBM\n    m2 = lgb.LGBMClassifier(**PARAMS_S1_LGB)\n    m2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n    p2_val = m2.predict_proba(X_val)[:, 1]\n    p2_test = m2.predict_proba(X_test)[:, 1]\n    \n    # Ensemble\n    fold_pred = W_S1['cat'] * p1_val + W_S1['lgb'] * p2_val\n    oof_prob_s1[val_idx] = fold_pred\n    test_prob_s1 += (W_S1['cat'] * p1_test + W_S1['lgb'] * p2_test) / N_FOLDS\n    \n    fold_auc = roc_auc_score(y_val, fold_pred)\n    fold_aucs.append(fold_auc)\n    print(f\"    AUC: {fold_auc:.5f}\")\n\n# Overall metrics\noverall_auc = roc_auc_score(y_binary, oof_prob_s1)\nprint(f\"\\n  Overall OOF AUC: {overall_auc:.5f} ¬± {np.std(fold_aucs):.5f}\")\n\n# Optimize threshold (by accuracy, for reference only)\nbest_acc = 0\nbest_thresh_acc = 0.5\nfor t in np.arange(0.2, 0.8, 0.005):\n    acc = accuracy_score(y_binary, (oof_prob_s1 > t).astype(int))\n    if acc > best_acc:\n        best_acc, best_thresh_acc = acc, t\n\nprint(f\"  Optimal Threshold (by ACC): {best_thresh_acc:.4f} (Accuracy: {best_acc:.4f})\")\n\n# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ default ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ tune RMSE ‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á\nbest_thresh = best_thresh_acc\n\n\n# ==========================================\n# STAGE 2: REGRESSION\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[4/5] STAGE 2: Amount Prediction (Spenders Only)\")\nprint(\"=\" * 60)\n\nmask_spenders = train_fe[TARGET] > 0\nX_reg = train_fe[mask_spenders][feature_cols].reset_index(drop=True)\ny_reg = train_fe[mask_spenders][TARGET].reset_index(drop=True)\ny_reg_log = np.log1p(y_reg)\n\nprint(f\"Training on {len(X_reg)} spenders\")\nprint(f\"Mean spending: ‡∏ø{y_reg.mean():.2f}, Median: ‡∏ø{y_reg.median():.2f}\")\n\n# Convert categoricals\nfor c in cat_features:\n    X_reg[c] = X_reg[c].astype('category')\n\noof_amount_s2 = np.zeros(len(X_reg))\ntest_amount_s2 = np.zeros(len(X_test))\nkf_reg = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\nfold_rmses = []\n\nfor fold, (tr_idx, val_idx) in enumerate(kf_reg.split(X_reg), 1):\n    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n    X_tr, X_val = X_reg.iloc[tr_idx], X_reg.iloc[val_idx]\n    y_tr, y_val = y_reg_log.iloc[tr_idx], y_reg_log.iloc[val_idx]\n    \n    # CatBoost\n    r1 = CatBoostRegressor(**PARAMS_S2_CAT, cat_features=cat_features)\n    r1.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=150)\n    p1_val = r1.predict(X_val)\n    p1_test = r1.predict(X_test)\n    \n    # LightGBM Main\n    r2 = lgb.LGBMRegressor(**PARAMS_S2_LGB_MAIN)\n    r2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(150, verbose=False)])\n    p2_val = r2.predict(X_val)\n    p2_test = r2.predict(X_test)\n    \n    # LightGBM Deep\n    r3 = lgb.LGBMRegressor(**PARAMS_S2_LGB_DEEP)\n    r3.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(150, verbose=False)])\n    p3_val = r3.predict(X_val)\n    p3_test = r3.predict(X_test)\n    \n    # Ensemble (log space)\n    fold_pred = (\n        W_S2['cat'] * p1_val + \n        W_S2['lgb_main'] * p2_val + \n        W_S2['lgb_deep'] * p3_val\n    )\n    oof_amount_s2[val_idx] = fold_pred\n    test_amount_s2 += (\n        W_S2['cat'] * p1_test + \n        W_S2['lgb_main'] * p2_test + \n        W_S2['lgb_deep'] * p3_test\n    ) / N_FOLDS\n    \n    fold_rmse = np.sqrt(mean_squared_error(y_val, fold_pred))\n    fold_rmses.append(fold_rmse)\n    print(f\"    RMSE (log): {fold_rmse:.5f}\")\n\n# Evaluation\nrmse_log = np.sqrt(mean_squared_error(y_reg_log, oof_amount_s2))\noof_amount_raw = np.expm1(oof_amount_s2)\nrmse_raw = np.sqrt(mean_squared_error(y_reg, oof_amount_raw))\n\nprint(f\"\\n  Overall OOF RMSE (log): {rmse_log:.5f} ¬± {np.std(fold_rmses):.5f}\")\nprint(f\"  Overall OOF RMSE (THB): ‡∏ø{rmse_raw:.2f}\")\n# ==========================================\n# [4.5] Tuning threshold by RMSE on spending_30d\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[4.5] Tuning classification threshold by RMSE on full train\")\nprint(\"=\" * 60)\n\n# y_true = ‡∏Ñ‡πà‡∏≤ spending ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á train\ny_true = train_fe[TARGET].values  # shape (104000,)\n\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á amount prediction ‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏±‡πâ‡∏á train (‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á)\namount_raw_full = np.zeros_like(y_true, dtype=float)\namount_raw_full[mask_spenders.values] = np.expm1(oof_amount_s2)\n\nbest_t = None\nbest_rmse = 1e18\n\nfor t in np.linspace(0.10, 0.90, 81):  # 0.10, 0.11, ..., 0.90\n    y_pred = np.where(oof_prob_s1 > t, amount_raw_full, 0.0)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    if rmse < best_rmse:\n        best_rmse = rmse\n        best_t = t\n\nprint(f\"  Best threshold by RMSE: {best_t:.4f}  (Train RMSE: {best_rmse:.2f})\")\nprint(f\"  Previous threshold by ACC: {best_thresh_acc:.4f}\")\n\n# ‡πÉ‡∏ä‡πâ threshold ‡πÅ‡∏ö‡∏ö RMSE ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö test\nbest_thresh = best_t\n\n\n# ==========================================\n# FINAL PREDICTIONS\n# ==========================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"[5/5] Generating Final Predictions\")\nprint(\"=\" * 60)\n\n# Convert test predictions from log to raw\ntest_amount_raw = np.expm1(test_amount_s2)\n\n# Combine stages with threshold\nfinal_preds = np.where(test_prob_s1 > best_thresh, test_amount_raw, 0)\n\n# Safety clip\nfinal_preds = np.clip(final_preds, 0, 500000)\n\n# Statistics\nprint(f\"\\nPrediction Statistics:\")\nprint(f\"  Threshold: {best_thresh:.4f}\")\nprint(f\"  Predicted spenders: {np.sum(final_preds > 0):,} / {len(final_preds):,} ({100*np.sum(final_preds > 0)/len(final_preds):.2f}%)\")\nprint(f\"  Mean prediction: ‡∏ø{final_preds[final_preds > 0].mean():.2f}\")\nprint(f\"  Median prediction: ‡∏ø{np.median(final_preds[final_preds > 0]):.2f}\")\nprint(f\"  Max prediction: ‡∏ø{final_preds.max():.2f}\")\nprint(f\"  Total predicted revenue: ‡∏ø{final_preds.sum():,.2f}\")\n\n# Save submission\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'spending_30d': final_preds\n})\nsubmission.to_csv('submission_enhanced.csv', index=False)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úì Pipeline Complete! Submission saved as 'submission_enhanced.csv'\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T22:33:51.893533Z","iopub.execute_input":"2025-11-24T22:33:51.893827Z","iopub.status.idle":"2025-11-24T23:10:24.014617Z","shell.execute_reply.started":"2025-11-24T22:33:51.893808Z","shell.execute_reply":"2025-11-24T23:10:24.013718Z"}},"outputs":[{"name":"stdout","text":"============================================================\nADVANCED TWO-STAGE PREDICTION PIPELINE (GPU ACCELERATED)\n============================================================\n\nüîç Checking GPU availability...\n‚úì GPU Detected: Tesla T4, 15360 MiB\nTesla T4, 15360 MiB\n\n[1/5] Loading data...\nTrain shape: (104000, 35), Test shape: (25889, 34)\n\n[2/5] Creating advanced features...\nTotal features: 73\nCategorical features: 7\n\n============================================================\n[3/5] STAGE 1: Spender Classification\n============================================================\nPositive class rate: 0.5180\nScale pos weight: 0.9306\n\n  Fold 1/7\n    AUC: 0.78826\n\n  Fold 2/7\n    AUC: 0.78667\n\n  Fold 3/7\n    AUC: 0.78297\n\n  Fold 4/7\n    AUC: 0.78354\n\n  Fold 5/7\n    AUC: 0.78035\n\n  Fold 6/7\n    AUC: 0.78393\n\n  Fold 7/7\n    AUC: 0.78375\n\n  Overall OOF AUC: 0.78384 ¬± 0.00238\n  Optimal Threshold (by ACC): 0.4700 (Accuracy: 0.7198)\n\n============================================================\n[4/5] STAGE 2: Amount Prediction (Spenders Only)\n============================================================\nTraining on 53868 spenders\nMean spending: ‡∏ø20019.98, Median: ‡∏ø2297.29\n\n  Fold 1/7\n    RMSE (log): 0.22017\n\n  Fold 2/7\n    RMSE (log): 0.21894\n\n  Fold 3/7\n    RMSE (log): 0.22386\n\n  Fold 4/7\n    RMSE (log): 0.20170\n\n  Fold 5/7\n    RMSE (log): 0.22873\n\n  Fold 6/7\n    RMSE (log): 0.19174\n\n  Fold 7/7\n    RMSE (log): 0.19488\n\n  Overall OOF RMSE (log): 0.21189 ¬± 0.01385\n  Overall OOF RMSE (THB): ‡∏ø14034.59\n\n============================================================\n[4.5] Tuning classification threshold by RMSE on full train\n============================================================\n  Best threshold by RMSE: 0.1000  (Train RMSE: 10100.63)\n  Previous threshold by ACC: 0.4700\n\n============================================================\n[5/5] Generating Final Predictions\n============================================================\n\nPrediction Statistics:\n  Threshold: 0.1000\n  Predicted spenders: 25,889 / 25,889 (100.00%)\n  Mean prediction: ‡∏ø11458.63\n  Median prediction: ‡∏ø1009.45\n  Max prediction: ‡∏ø282003.88\n  Total predicted revenue: ‡∏ø296,652,593.67\n\n============================================================\n‚úì Pipeline Complete! Submission saved as 'submission_enhanced.csv'\n============================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}