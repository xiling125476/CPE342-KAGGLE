{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\n\n# CRITICAL: Fix CuPy circular import BEFORE any imports\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n\n# Disable CuPy in Dask (which LightGBM tries to import)\nos.environ['DASK_ARRAY__BACKEND__CUPY'] = '0'\n\n# Mock cupy to prevent import\nsys.modules['cupy'] = None\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, QuantileTransformer\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom functools import lru_cache\n\n# Try importing with error handling\ntry:\n    from imblearn.over_sampling import BorderlineSMOTE\n    SMOTE_AVAILABLE = True\nexcept ImportError:\n    SMOTE_AVAILABLE = False\n    print(\"‚ö†Ô∏è  BorderlineSMOTE not available, will skip oversampling\")\n\ntry:\n    from xgboost import XGBClassifier\n    XGB_AVAILABLE = True\nexcept ImportError:\n    XGB_AVAILABLE = False\n    print(\"‚ö†Ô∏è  XGBoost not available\")\n\ntry:\n    from lightgbm import LGBMClassifier\n    LGBM_AVAILABLE = True\nexcept ImportError:\n    LGBM_AVAILABLE = False\n    print(\"‚ö†Ô∏è  LightGBM not available\")\n\ntry:\n    from catboost import CatBoostClassifier\n    CAT_AVAILABLE = True\nexcept ImportError:\n    CAT_AVAILABLE = False\n    print(\"‚ö†Ô∏è  CatBoost not available\")\n\nimport joblib\n\nprint(\"=\"*80)\nprint(\"üöÄ OPTIMIZED ML PIPELINE - PLAYER SEGMENTATION (Segment-Specific FE)\")\nprint(\"=\"*80)\n\n# ===========================\n# 1. DATA LOADING (Optimized)\n# ===========================\n# IMPORTANT: Use relative paths for execution environment\nTRAIN_PATH = '/kaggle/input/datasettask2/train.csv'\nTEST_PATH = '/kaggle/input/datasettask2/test.csv'\nSUBMISSION_TEMPLATE = '/kaggle/input/datasettask2/sample_submission.csv' \n\n# Utility to create dummy submission file if it doesn't exist\ntry:\n    df_temp_test = pd.read_csv(TEST_PATH, low_memory=False)\n    df_sub_dummy = pd.DataFrame({'id': df_temp_test['id'], 'task2': 'Segment 0'})\n    df_sub_dummy.to_csv(SUBMISSION_TEMPLATE, index=False)\n    del df_temp_test, df_sub_dummy\nexcept Exception as e:\n    print(f\"Could not create dummy submission file: {e}\")\n\ndf_train = pd.read_csv(TRAIN_PATH, low_memory=False)\ndf_test = pd.read_csv(TEST_PATH, low_memory=False)\ndf_sub = pd.read_csv(SUBMISSION_TEMPLATE)\nprint(\"‚úÖ Data loaded successfully.\")\n\nprint(f\"\\nüìä Train shape: {df_train.shape}\")\nprint(f\"üìä Test shape: {df_test.shape}\")\n\n# ===========================\n# 2. INITIAL SETUP\n# ===========================\nTARGET_VAR = 'segment'\nNON_FEATURE_COLS = ['id', 'player_id']\n\n# Vectorized operations\nX = df_train.drop(columns=[TARGET_VAR] + NON_FEATURE_COLS, errors='ignore')\ny = df_train[TARGET_VAR]\nX_test = df_test.drop(columns=NON_FEATURE_COLS, errors='ignore')\n\n# Handle NaN in target (vectorized)\nvalid_idx = y.notna()\nX = X[valid_idx].reset_index(drop=True)\ny = y[valid_idx].reset_index(drop=True)\n\n# Encode target\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nclass_dist = pd.Series(y_encoded).value_counts().sort_index()\nprint(f\"\\nüìä Class Distribution:\")\nfor i, count in enumerate(class_dist):\n    print(f\"    Segment {i}: {count:,} ({count/len(X)*100:.1f}%)\")\n\n# ===========================\n# 3. OPTIMIZED DATA CLEANING\n# ===========================\nprint(\"\\nüßπ Cleaning data (optimized)...\")\n\nnumerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n# Vectorized imputation - compute medians once\nnumerical_medians = X[numerical_cols].median()\n\n# Apply to train\nX[numerical_cols] = X[numerical_cols].fillna(numerical_medians)\n\n# Apply to test (reuse computed medians)\nX_test[numerical_cols] = X_test[numerical_cols].fillna(numerical_medians)\n\n# Categorical imputation (vectorized)\nX[categorical_cols] = X[categorical_cols].fillna('Missing_Value')\nX_test[categorical_cols] = X_test[categorical_cols].fillna('Missing_Value')\n\nprint(\"‚úÖ Data cleaned with vectorized operations\")\n\n# ===========================\n# 4. OPTIMIZED FEATURE ENGINEERING (Segment-Specific Decoupling)\n# ===========================\n\ndef create_ultimate_features(df):\n    \"\"\"\n    Optimized feature creation focusing on explicit Segment-Specific Decoupling.\n    \"\"\"\n    result = df.copy()\n    \n    # Precompute common terms\n    eps = 1e-6\n    account_age_safe = result['account_age_days'] + eps\n    playtime_safe = result['total_playtime_hours'] + eps\n    \n    new_features = {}\n\n    # --- S0: Casual Segment Features (Focus: Low Commitment, Recency) ---\n    \n    # 1. Recency Index (Recency / Age)\n    new_features['S0_Recency_Index'] = result['days_since_last_login'] / account_age_safe\n    \n    # 2. Low Commitment Ratio (Session Duration / Login Streak)\n    new_features['S0_Low_Commitment_Ratio'] = result['avg_session_duration'] / (result['login_streak'] + eps)\n    \n    # 3. Non-Competitive Focus\n    new_features['S0_Non_Competitive_Focus'] = 1 - result['ranked_participation_rate']\n    \n    # 4. Low Spending Binary (Binary flag for non-payers)\n    new_features['S0_Low_Spending_Binary'] = (result['total_spending_thb'] == 0).astype(np.int8)\n    \n    # 5. Play Intermittence Score\n    new_features['S0_Play_Intermittence'] = result['days_since_last_login'] * (1 / (result['play_frequency'] + eps))\n    \n    # 6. Low Playtime Per Day\n    new_features['S0_Playtime_Per_Day_Inv'] = 1.0 / (result['total_playtime_hours'] / account_age_safe + eps)\n\n\n    # --- S1: Grinder Segment Features (Focus: Dedication, Competitive, Progress) ---\n    \n    # 1. Grinder Intensity (Login/Playtime relative to Age)\n    new_features['S1_Grinder_Intensity'] = (result['login_streak'] * result['play_frequency']) / account_age_safe\n    \n    # 2. Ranked Dedication Index (Participation * Win Rate)\n    new_features['S1_Ranked_Dedication_Index'] = result['ranked_participation_rate'] * result['win_rate_ranked']\n    \n    # 3. Progression Pace (Achievement * Speed of Progression)\n    new_features['S1_Progression_Pace'] = result['achievement_completion_rate'] * result['speed_of_progression']\n    \n    # 4. Competitive Ranked Hours (Total Playtime * Ranked %)\n    new_features['S1_Competitive_Ranked_Hours'] = result['total_playtime_hours'] * result['ranked_participation_rate']\n    \n    # 5. Consistent Play Hours (Total Playtime / Streak)\n    new_features['S1_Consistent_Play_Hours'] = result['total_playtime_hours'] / (result['login_streak'] + eps)\n    \n    # 6. Tournament Engagement\n    new_features['S1_Tournament_Engagement'] = result['tournament_entries'] / playtime_safe\n\n\n    # --- S2: Social Segment Features (Focus: Friends, Chat, Team Play) ---\n    \n    # 1. Network Reach Score (Friend Count * Chat Score)\n    new_features['S2_Network_Reach_Score'] = result['friend_count'] * result['chat_activity_score']\n    \n    # 2. Gifting Generosity (Gifts / Friend Count)\n    new_features['S2_Gifting_Generosity'] = result['gifts_sent_received'] / (result['friend_count'] + eps)\n    \n    # 3. Team Play Ratio (Team Play % / Ranked %)\n    new_features['S2_Team_Play_Ratio'] = result['team_play_percentage'] / (result['ranked_participation_rate'] + eps)\n    \n    # 4. Chat Per Playtime\n    new_features['S2_Chat_Per_Playtime'] = result['chat_activity_score'] / playtime_safe\n    \n    # 5. Social Dominance (Invites/Gifts * Team Play)\n    new_features['S2_Social_Dominance'] = (result['friend_invites_sent'] + result['gifts_sent_received']) * result['team_play_percentage']\n    \n    # 6. Non-Monetary Social Score\n    new_features['S2_Non_Monetary_Social'] = result['friend_count'] + result['gifts_sent_received'] + result['friend_invites_sent']\n\n\n    # --- S3: Whale Segment Features (Focus: Spending, VIP, Value) ---\n    \n    # 1. Log Spending (Key Transformation)\n    new_features['S3_Log_Spending'] = np.log1p(result['total_spending_thb'])\n    \n    # 2. Monthly Spending Power\n    new_features['S3_Monthly_Spending_Power'] = result['avg_monthly_spending'] * result['spending_frequency']\n    \n    # 3. VIP Spending Ratio\n    new_features['S3_VIP_Spending_Ratio'] = result['vip_tier'] * result['total_spending_thb']\n    \n    # 4. Investment Return Rate (Spending / Playtime)\n    new_features['S3_Investment_Return_Rate'] = result['total_spending_thb'] / playtime_safe\n    \n    # 5. Discount Responsiveness\n    new_features['S3_Discount_Responsiveness'] = result['responds_to_discounts'] * result['spending_frequency']\n    \n    # 6. High Value Collection (Rare Items * Collection Progress)\n    new_features['S3_High_Value_Collection'] = result['rare_items_count'] * result['collection_progress']\n    \n    # 7. Whale Per Day (Spending / Age)\n    new_features['S3_Whale_Per_Day'] = result['total_spending_thb'] / account_age_safe\n\n\n    # Assign all at once (faster than individual assignments)\n    for col, values in new_features.items():\n        result[col] = values\n        \n    # --- Cleanup for new ratio features ---\n    cols_to_clean = list(new_features.keys())\n    # Handle inf/NaN and fill with 0 (a safe assumption for ratio features derived from 0 denominators)\n    result[cols_to_clean].replace([np.inf, -np.inf], np.nan, inplace=True)\n    result[cols_to_clean].fillna(0, inplace=True) \n    \n    return result\n\nX = create_ultimate_features(X)\nX_test = create_ultimate_features(X_test)\n\nprint(f\"‚úÖ Features created. Total numerical: {X.select_dtypes(include=np.number).shape[1]}\")\n\n# ===========================\n# 5. OPTIMIZED ENCODING\n# ===========================\nprint(\"\\nüîÑ Encoding categorical features (optimized)...\")\n\ncategorical_cols_final = X.select_dtypes(include='object').columns.tolist()\n\n# Efficient concatenation\ncombined_df = pd.concat([X, X_test], axis=0, ignore_index=True, copy=False)\n\n# One-Hot Encoding with optimized parameters\ncombined_df = pd.get_dummies(\n    combined_df, \n    columns=categorical_cols_final, \n    drop_first=True, \n    dummy_na=False,\n    dtype=np.int8  # Use int8 instead of int64 to save memory\n)\n\n# Split efficiently\nsplit_idx = len(X)\nX = combined_df.iloc[:split_idx].copy()\nX_test_temp = combined_df.iloc[split_idx:].copy()\n\n# Align columns efficiently\nmissing_cols = set(X.columns) - set(X_test_temp.columns)\nif missing_cols:\n    for c in missing_cols:\n        X_test_temp[c] = 0\n\nX_test = X_test_temp[X.columns].copy()\n\nprint(f\"‚úÖ Final dataset: {X.shape[1]} features\")\n\n# Convert to numpy arrays for faster model training\nX_array = X.values\nX_test_array = X_test.values\n\n# ===========================\n# 6. OPTIMIZED CLASS WEIGHTS\n# ===========================\nprint(f\"\\n{'='*80}\")\nprint(\"‚öñÔ∏è COMPUTING OPTIMAL CLASS WEIGHTS\")\nprint(f\"{'='*80}\")\n\n# Vectorized class weight calculation\nclass_counts = np.bincount(y_encoded)\ntotal = len(y_encoded)\nnum_classes = len(class_counts)\n\nclass_weights = {i: total / (num_classes * count) for i, count in enumerate(class_counts)}\n\n# Custom adjustments (vectorized)\nif num_classes >= 4:\n    # Adjusted weights to slightly boost S1 and S2 (Grinder, Social)\n    adjustments = [1.00, 1.08, 1.20, 1.12]\n    for i, adj in enumerate(adjustments):\n        class_weights[i] *= adj\n\nprint(\"\\nüìä Optimized class weights:\")\nfor i, w in class_weights.items():\n    print(f\"    Segment {i}: {w:.3f}\")\n\n# ===========================\n# 7. OPTIMIZED MODEL ENSEMBLE\n# ===========================\nprint(f\"\\n{'='*80}\")\nprint(\"üèóÔ∏è BUILDING OPTIMIZED ENSEMBLE\")\nprint(f\"{'='*80}\")\n\n# Build ensemble based on available libraries\nbase_models = []\n\nif XGB_AVAILABLE:\n    base_models.extend([\n        ('xgb_deep', XGBClassifier(\n            n_estimators=550, max_depth=9, learning_rate=0.028,\n            tree_method='hist', use_label_encoder=False,\n            random_state=42, n_jobs=-1, eval_metric='mlogloss',\n            device='cpu'\n        )),\n        ('xgb_wide', XGBClassifier(\n            n_estimators=550, max_depth=6, learning_rate=0.028,\n            tree_method='hist', use_label_encoder=False,\n            random_state=43, n_jobs=-1, eval_metric='mlogloss',\n            device='cpu'\n        ))\n    ])\n    print(\"‚úì XGBoost models added\")\n\nif LGBM_AVAILABLE:\n    base_models.append(\n        ('lgbm', LGBMClassifier(\n            n_estimators=550, num_leaves=50, learning_rate=0.028,\n            class_weight=class_weights, random_state=42,\n            n_jobs=-1, verbose=-1, force_col_wise=True,\n            device='cpu'\n        ))\n    )\n    print(\"‚úì LightGBM model added\")\n\nif CAT_AVAILABLE:\n    base_models.append(\n        ('cat', CatBoostClassifier(\n            n_estimators=550, depth=9, learning_rate=0.028,\n            class_weights=list(class_weights.values()),\n            random_seed=42, verbose=0, thread_count=-1,\n            task_type='CPU', bootstrap_type='Bernoulli'\n        ))\n    )\n    print(\"‚úì CatBoost model added\")\n\n# Always add sklearn models (most stable)\nbase_models.extend([\n    ('rf', RandomForestClassifier(\n        n_estimators=450, max_depth=19,\n        class_weight=class_weights, random_state=42,\n        n_jobs=-1, max_features='sqrt'\n    )),\n    ('et', ExtraTreesClassifier(\n        n_estimators=450, max_depth=19,\n        class_weight=class_weights, random_state=44,\n        n_jobs=-1, max_features='sqrt'\n    ))\n])\nprint(\"‚úì RandomForest & ExtraTrees models added\")\n\nif len(base_models) == 0:\n    raise RuntimeError(\"No models available! Please install at least one of: xgboost, lightgbm, catboost\")\n\nprint(f\"\\nüìä Total models in ensemble: {len(base_models)}\")\n\n# Meta model - use LightGBM if available, else RandomForest\nif LGBM_AVAILABLE:\n    meta_base = LGBMClassifier(\n        n_estimators=300, num_leaves=35, learning_rate=0.035,\n        random_state=42, n_jobs=-1, verbose=-1, force_col_wise=True,\n        device='cpu'\n    )\n    print(\"‚úì Meta-model: LightGBM\")\nelse:\n    meta_base = RandomForestClassifier(\n        n_estimators=300, max_depth=15,\n        class_weight=class_weights, random_state=42, n_jobs=-1\n    )\n    print(\"‚úì Meta-model: RandomForest (fallback)\")\n\n# ===========================\n# 8. OPTIMIZED TRAINING\n# ===========================\nprint(f\"\\n{'='*80}\")\nprint(\"üéØ TRAINING PHASE - Optimized CV\")\nprint(f\"{'='*80}\")\n\nN_SPLITS = 5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nn_classes = len(le.classes_)\n\n# Preallocate arrays (more efficient)\noof_preds_L1 = np.zeros((len(X_array), len(base_models) * n_classes), dtype=np.float32)\ntest_preds_L1 = np.zeros((len(X_test_array), len(base_models) * n_classes), dtype=np.float32)\n\nfold_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_array, y_encoded)):\n    print(f\"\\n{'‚îÄ'*80}\")\n    print(f\"üìç Fold {fold+1}/{N_SPLITS}\")\n    print(f\"{'‚îÄ'*80}\")\n    \n    X_train, X_val = X_array[train_idx], X_array[val_idx]\n    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n    \n    # BorderlineSMOTE with error handling\n    if SMOTE_AVAILABLE:\n        try:\n            smote = BorderlineSMOTE(random_state=42, k_neighbors=6, kind='borderline-1', n_jobs=-1)\n            X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n            print(f\"  ‚úì BorderlineSMOTE: {len(X_train):,} -> {len(X_train_res):,}\")\n        except Exception as e:\n            X_train_res, y_train_res = X_train, y_train\n            print(f\"  ‚ö† BorderlineSMOTE skipped: {str(e)[:50]}\")\n    else:\n        X_train_res, y_train_res = X_train, y_train\n        print(f\"  ‚ö† SMOTE not available, using original data\")\n    \n    # Train base models with progress\n    print(f\"\\n  Training base models:\")\n    for i, (name, model) in enumerate(base_models):\n        model.fit(X_train_res, y_train_res)\n        \n        # Predict probabilities\n        val_probs = model.predict_proba(X_val)\n        oof_preds_L1[val_idx, i*n_classes:(i+1)*n_classes] = val_probs\n        \n        test_probs = model.predict_proba(X_test_array)\n        test_preds_L1[:, i*n_classes:(i+1)*n_classes] += test_probs / N_SPLITS\n        \n        print(f\"    ‚Ä¢ {name:12s} ‚úì\")\n    \n    # Fold evaluation (vectorized)\n    fold_preds = oof_preds_L1[val_idx].reshape(len(val_idx), len(base_models), -1).mean(axis=1).argmax(axis=1)\n    fold_f1 = f1_score(y_val, fold_preds, average='macro')\n    fold_scores.append(fold_f1)\n    \n    print(f\"\\n  üìä Fold F1: {fold_f1:.4f}\")\n\n# ===========================\n# 9. OPTIMIZED META-MODEL\n# ===========================\nprint(f\"\\n{'='*80}\")\nprint(\"üéì TRAINING CALIBRATED META-MODEL\")\nprint(f\"{'='*80}\")\n\n# Fit scaler on float32 data\nscaler = QuantileTransformer(output_distribution='normal', random_state=42)\nX_meta_scaled = scaler.fit_transform(oof_preds_L1)\nX_test_meta_scaled = scaler.transform(test_preds_L1)\n\n# Train meta-model with calibration\nmeta_model = CalibratedClassifierCV(meta_base, method='isotonic', cv=3, n_jobs=-1)\nmeta_model.fit(X_meta_scaled, y_encoded)\n\n# Final predictions\noof_preds_final = meta_model.predict(X_meta_scaled)\nfinal_predictions = meta_model.predict(X_test_meta_scaled)\n\n# ===========================\n# 10. EVALUATION\n# ===========================\ncv_f1 = f1_score(y_encoded, oof_preds_final, average='macro')\n\nprint(f\"\\n{'='*80}\")\nprint(f\"‚ú® FINAL RESULTS\")\nprint(f\"{'='*80}\")\nprint(f\"\\nüéØ Final CV F1-Macro: {cv_f1:.4f}\")\n\nprint(f\"\\n{'='*80}\")\nprint(classification_report(\n    y_encoded, oof_preds_final,\n    target_names=[f\"Segment {c}\" for c in le.classes_],\n    digits=4\n))\n\n# Confusion matrix\ncm = confusion_matrix(y_encoded, oof_preds_final)\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nprint(\"\\nüîç Normalized Confusion Matrix:\\n\")\nprint(\"    \\t\\t Predicted:\", \" \".join([f\"S{i}\" for i in range(n_classes)]))\nfor i, row in enumerate(cm_norm):\n    print(f\"  True S{i}:\\t\\t\", \" \".join([f\"{v:.2f}\" for v in row]))\n\n# ===========================\n# 11. SUBMISSION\n# ===========================\nfinal_predictions_decoded = le.inverse_transform(final_predictions)\ndf_sub['task2'] = final_predictions_decoded\nsubmission = df_sub[['id', 'task2']]\n\nsubmission.to_csv('submission_task2_optimized_v2.csv', index=False)\n\nprint(f\"\\n{'='*80}\")\nprint(\"‚úÖ SUBMISSION SAVED: 'submission_task2_optimized_v2.csv'\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}