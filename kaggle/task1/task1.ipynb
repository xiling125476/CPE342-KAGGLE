{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13808898,"sourceType":"datasetVersion","datasetId":8792970}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======================================================================\n# TASK 1 — is_cheater\n# FE v2 + Polynomial Interaction Features + LGBM + XGB + CatBoost + Stacking + F2 Tuning\n# ======================================================================\n\n!pip install catboost xgboost lightgbm --quiet\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nSEED = 42\nN_FOLDS = 5\nTARGET = \"is_cheater\"\n\n\n# ============================================================\n# 1. Load data\n# ============================================================\nTRAIN_PATH = \"/kaggle/input/dataset22/train.csv\"\nTEST_PATH  = \"/kaggle/input/dataset22/test.csv\"\n\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\n\nprint(\"Train shape:\", train.shape, \" Test shape:\", test.shape)\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:00:51.206154Z","iopub.execute_input":"2025-11-28T14:00:51.207060Z","iopub.status.idle":"2025-11-28T14:00:55.165389Z","shell.execute_reply.started":"2025-11-28T14:00:51.207032Z","shell.execute_reply":"2025-11-28T14:00:55.164502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install catboost lightgbm xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T13:56:10.435284Z","iopub.execute_input":"2025-11-28T13:56:10.435628Z","iopub.status.idle":"2025-11-28T13:56:14.647341Z","shell.execute_reply.started":"2025-11-28T13:56:10.435586Z","shell.execute_reply":"2025-11-28T13:56:14.646471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# 2. FE v2 + Polynomial Interaction Features (auto)\n# ============================================================\ndef fe_v2_poly(train_df: pd.DataFrame, test_df: pd.DataFrame):\n    train_df = train_df.copy()\n    test_df = test_df.copy()\n    \n    # ---------- basic missing handling ----------\n    full = pd.concat([train_df.drop(columns=[TARGET]), test_df], axis=0, ignore_index=True)\n    \n    for col in full.columns:\n        if full[col].dtype == \"O\":\n            full[col] = full[col].fillna(\"missing\")\n        else:\n            full[col] = full[col].fillna(full[col].median())\n    \n    # reconstruct back\n    train_features = full.iloc[:len(train_df)].copy()\n    test_features  = full.iloc[len(train_df):].copy()\n    \n    # ---------- detect types ----------\n    ignore_cols = [\"id\"]\n    num_cols = [c for c in train_features.columns if c not in ignore_cols and train_features[c].dtype != \"O\"]\n    cat_cols = [c for c in train_features.columns if c not in ignore_cols and train_features[c].dtype == \"O\"]\n    \n    print(\"Numeric cols:\", len(num_cols), \" | Categorical cols:\", len(cat_cols))\n    \n    # ---------- base numeric transforms ----------\n    for df in [train_features, test_features]:\n        num_data = df[num_cols].astype(float)\n        \n        # row-wise stats\n        df[\"num_mean\"] = num_data.mean(axis=1)\n        df[\"num_std\"]  = num_data.std(axis=1)\n        df[\"num_max\"]  = num_data.max(axis=1)\n        df[\"num_min\"]  = num_data.min(axis=1)\n        \n        # log / sqrt for skewed features\n        for col in num_cols:\n            col_clip = num_data[col].clip(lower=0)\n            if (col_clip > 0).mean() > 0.3:\n                df[f\"{col}_log1p\"] = np.log1p(col_clip)\n            df[f\"{col}_sqrt\"] = np.sqrt(col_clip)\n    \n    # update numeric list after new features\n    num_cols_all = [c for c in train_features.columns if c not in ignore_cols and train_features[c].dtype != \"O\"]\n    \n    # ---------- Polynomial-style interaction (auto) ----------\n    # ใช้ top-K numeric features ที่ variance สูงสุดมาทำ pairwise interaction\n    K = min(15, len(num_cols_all))  # limit เพื่อกัน feature ระเบิด\n    \n    var_series = train_features[num_cols_all].var().sort_values(ascending=False)\n    top_num_cols = list(var_series.index[:K])\n    print(\"Top numeric for interactions:\", top_num_cols)\n    \n    def add_poly_interactions(df, cols):\n        # สร้าง sum, diff, product, ratio (ระวังหาร 0)\n        for i in range(len(cols)):\n            for j in range(i+1, len(cols)):\n                c1, c2 = cols[i], cols[j]\n                c1v = df[c1].astype(float)\n                c2v = df[c2].astype(float)\n                df[f\"{c1}_plus_{c2}\"]  = c1v + c2v\n                df[f\"{c1}_minus_{c2}\"] = c1v - c2v\n                df[f\"{c1}_mul_{c2}\"]   = c1v * c2v\n                df[f\"{c1}_ratio_{c2}\"] = c1v / (c2v.abs() + 1e-3)\n    \n    for df in [train_features, test_features]:\n        add_poly_interactions(df, top_num_cols)\n    \n    # ---------- LabelEncode categorical ----------\n    full2 = pd.concat([train_features, test_features], axis=0, ignore_index=True)\n    cat_cols = [c for c in full2.columns if full2[c].dtype == \"O\"]\n    print(\"Detected categorical columns for LabelEncoder:\", cat_cols)\n    \n    for col in cat_cols:\n        le = LabelEncoder()\n        full2[col] = le.fit_transform(full2[col].astype(str))\n    \n    train_final = full2.iloc[:len(train_df)].copy()\n    test_final  = full2.iloc[len(train_df):].copy()\n    \n    # add target back\n    train_final[TARGET] = train_df[TARGET].values\n    \n    # final feature list\n    features = [c for c in train_final.columns if c not in [\"id\", TARGET]]\n    \n    return train_final, test_final, features\n\nprint(\"Applying FE v2 + Polynomial interactions ...\")\ntrain_fe, test_fe, features = fe_v2_poly(train, test)\nprint(\"Num features after FE:\", len(features))\n\nX = train_fe[features]\ntrain_fe[TARGET] = pd.to_numeric(train_fe[TARGET], errors='coerce').fillna(0).astype(int)\ny = train_fe[TARGET].values\nX_test = test_fe[features]\n\npos_rate = y.mean()\nprint(f\"Positive class rate: {pos_rate:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:00:56.616423Z","iopub.execute_input":"2025-11-28T14:00:56.616749Z","iopub.status.idle":"2025-11-28T14:00:59.293015Z","shell.execute_reply.started":"2025-11-28T14:00:56.616722Z","shell.execute_reply":"2025-11-28T14:00:59.292246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# 3. Model definitions (Ultra: 3 trees + stacking)\n# ============================================================\n\nnp.random.seed(SEED)\nLGB_PARAMS = dict(\n    objective=\"binary\",\n    metric=\"binary_logloss\",\n    learning_rate=0.03,\n    num_leaves=80,\n    max_depth=-1,\n    min_child_samples=40,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    reg_lambda=2.0,\n    reg_alpha=1.0,\n    random_state=SEED,\n    n_estimators=4000,\n    verbose=-1\n)\n\nXGB_PARAMS = dict(\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    learning_rate=0.03,\n    max_depth=8,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_weight=4,\n    reg_lambda=2.0,\n    reg_alpha=2.0,\n    tree_method=\"gpu_hist\",  # ถ้าไม่มี GPU ให้เปลี่ยนเป็น \"hist\"\n    random_state=SEED,\n    n_estimators=4000\n)\n\nCAT_PARAMS = dict(\n    loss_function=\"Logloss\",\n    eval_metric=\"Logloss\",\n    learning_rate=0.03,\n    depth=7,\n    l2_leaf_reg=4.0,\n    random_strength=1.5,\n    iterations=4000,\n    task_type=\"GPU\",  # ถ้าไม่มี GPU ให้ลบบรรทัดนี้\n    verbose=False,\n    random_seed=SEED\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:02:26.601375Z","iopub.execute_input":"2025-11-28T14:02:26.602145Z","iopub.status.idle":"2025-11-28T14:02:26.607698Z","shell.execute_reply.started":"2025-11-28T14:02:26.602118Z","shell.execute_reply":"2025-11-28T14:02:26.607035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# 4. K-Fold + OOF for stacking\n# ============================================================\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\noof_lgb = np.zeros(len(X))\noof_xgb = np.zeros(len(X))\noof_cat = np.zeros(len(X))\n\ntest_lgb = np.zeros(len(X_test))\ntest_xgb = np.zeros(len(X_test))\ntest_cat = np.zeros(len(X_test))\n\ndef f2_score(y_true, y_prob, thr):\n    y_pred = (y_prob > thr).astype(int)\n    return fbeta_score(y_true, y_pred, beta=2)\n\nfor fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n========== Fold {fold}/{N_FOLDS} ==========\")\n    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n    \n    # LightGBM\n    m_lgb = lgb.LGBMClassifier(**LGB_PARAMS)\n    m_lgb.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n    )\n    oof_lgb[va_idx] = m_lgb.predict_proba(X_va)[:, 1]\n    test_lgb += m_lgb.predict_proba(X_test)[:, 1] / N_FOLDS\n    \n    # XGBoost\n    m_xgb = xgb.XGBClassifier(**XGB_PARAMS)\n    m_xgb.fit(\n        X_tr, y_tr,\n        eval_set=[(X_va, y_va)],\n        early_stopping_rounds=200,\n        verbose=False\n    )\n    oof_xgb[va_idx] = m_xgb.predict_proba(X_va)[:, 1]\n    test_xgb += m_xgb.predict_proba(X_test)[:, 1] / N_FOLDS\n    \n    # CatBoost\n    m_cat = CatBoostClassifier(**CAT_PARAMS)\n    m_cat.fit(\n        X_tr, y_tr,\n        eval_set=(X_va, y_va),\n        early_stopping_rounds=200\n    )\n    oof_cat[va_idx] = m_cat.predict_proba(X_va)[:, 1]\n    test_cat += m_cat.predict_proba(X_test)[:, 1] / N_FOLDS\n    \n    # quick F2 check @0.5\n    for name, oof_part in [(\"lgb\", oof_lgb), (\"xgb\", oof_xgb), (\"cat\", oof_cat)]:\n        f2 = f2_score(y, oof_part, 0.5)\n        print(f\"  current F2 ({name}) @0.5 = {f2:.4f}\", end=\" | \")\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:02:31.470876Z","iopub.execute_input":"2025-11-28T14:02:31.471555Z","iopub.status.idle":"2025-11-28T14:14:32.181957Z","shell.execute_reply.started":"2025-11-28T14:02:31.471528Z","shell.execute_reply":"2025-11-28T14:14:32.181042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# 5. Stacking Meta-Model (Logistic Regression)\n# ============================================================\nmeta_train = np.vstack([oof_lgb, oof_xgb, oof_cat]).T\nmeta_test  = np.vstack([test_lgb, test_xgb, test_cat]).T\n\nmeta_clf = LogisticRegression(\n    C=2.0,\n    max_iter=1000,\n    class_weight=\"balanced\",\n    random_state=SEED\n)\nmeta_clf.fit(meta_train, y)\n\noof_meta = meta_clf.predict_proba(meta_train)[:, 1]\ntest_meta = meta_clf.predict_proba(meta_test)[:, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:14:32.183438Z","iopub.execute_input":"2025-11-28T14:14:32.183726Z","iopub.status.idle":"2025-11-28T14:14:32.473351Z","shell.execute_reply.started":"2025-11-28T14:14:32.183707Z","shell.execute_reply":"2025-11-28T14:14:32.472669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# 6. Threshold tuning on OOF (F2)\n# ============================================================\nprint(\"\\n========== Threshold tuning (F2) on OOF ==========\")\nbest_thr = 0.5\nbest_f2 = 0.0\nfor t in np.arange(0.05, 0.95, 0.01):\n    f2 = f2_score(y, oof_meta, t)\n    if f2 > best_f2:\n        best_f2 = f2\n        best_thr = t\n\nprint(f\"Best threshold (meta): {best_thr:.3f}  |  F2 = {best_f2:.5f}\")\n\n# optional: F2 of each model at best_thr\nfor name, oof_part in [(\"lgb\", oof_lgb), (\"xgb\", oof_xgb), (\"cat\", oof_cat), (\"meta\", oof_meta)]:\n    f2 = f2_score(y, oof_part, best_thr)\n    print(f\"  F2 {name:5s} @ {best_thr:.3f} = {f2:.5f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:14:32.473870Z","iopub.execute_input":"2025-11-28T14:14:32.474054Z","iopub.status.idle":"2025-11-28T14:14:36.390052Z","shell.execute_reply.started":"2025-11-28T14:14:32.474039Z","shell.execute_reply":"2025-11-28T14:14:36.389391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# 7. Final prediction & submission\n# ============================================================\nfinal_pred = (test_meta > best_thr).astype(int)\n\nsub = pd.DataFrame({\n    \"id\": test[\"id\"],\n    \"is_cheater\": final_pred\n})\nsub.to_csv(\"submission_task1_poly_ultra.csv\", index=False)\nprint(\"\\nSaved: submission_task1_poly_ultra.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:14:36.391211Z","iopub.execute_input":"2025-11-28T14:14:36.391392Z","iopub.status.idle":"2025-11-28T14:14:36.419278Z","shell.execute_reply.started":"2025-11-28T14:14:36.391377Z","shell.execute_reply":"2025-11-28T14:14:36.418741Z"}},"outputs":[],"execution_count":null}]}