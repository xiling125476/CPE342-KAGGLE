{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T22:33:51.893827Z",
     "iopub.status.busy": "2025-11-24T22:33:51.893533Z",
     "iopub.status.idle": "2025-11-24T23:10:24.014617Z",
     "shell.execute_reply": "2025-11-24T23:10:24.013718Z",
     "shell.execute_reply.started": "2025-11-24T22:33:51.893808Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADVANCED TWO-STAGE PREDICTION PIPELINE (GPU ACCELERATED)\n",
      "============================================================\n",
      "\n",
      "ðŸ” Checking GPU availability...\n",
      "âœ“ GPU Detected: Tesla T4, 15360 MiB\n",
      "Tesla T4, 15360 MiB\n",
      "\n",
      "[1/5] Loading data...\n",
      "Train shape: (104000, 35), Test shape: (25889, 34)\n",
      "\n",
      "[2/5] Creating advanced features...\n",
      "Total features: 73\n",
      "Categorical features: 7\n",
      "\n",
      "============================================================\n",
      "[3/5] STAGE 1: Spender Classification\n",
      "============================================================\n",
      "Positive class rate: 0.5180\n",
      "Scale pos weight: 0.9306\n",
      "\n",
      "  Fold 1/7\n",
      "    AUC: 0.78826\n",
      "\n",
      "  Fold 2/7\n",
      "    AUC: 0.78667\n",
      "\n",
      "  Fold 3/7\n",
      "    AUC: 0.78297\n",
      "\n",
      "  Fold 4/7\n",
      "    AUC: 0.78354\n",
      "\n",
      "  Fold 5/7\n",
      "    AUC: 0.78035\n",
      "\n",
      "  Fold 6/7\n",
      "    AUC: 0.78393\n",
      "\n",
      "  Fold 7/7\n",
      "    AUC: 0.78375\n",
      "\n",
      "  Overall OOF AUC: 0.78384 Â± 0.00238\n",
      "  Optimal Threshold (by ACC): 0.4700 (Accuracy: 0.7198)\n",
      "\n",
      "============================================================\n",
      "[4/5] STAGE 2: Amount Prediction (Spenders Only)\n",
      "============================================================\n",
      "Training on 53868 spenders\n",
      "Mean spending: à¸¿20019.98, Median: à¸¿2297.29\n",
      "\n",
      "  Fold 1/7\n",
      "    RMSE (log): 0.22017\n",
      "\n",
      "  Fold 2/7\n",
      "    RMSE (log): 0.21894\n",
      "\n",
      "  Fold 3/7\n",
      "    RMSE (log): 0.22386\n",
      "\n",
      "  Fold 4/7\n",
      "    RMSE (log): 0.20170\n",
      "\n",
      "  Fold 5/7\n",
      "    RMSE (log): 0.22873\n",
      "\n",
      "  Fold 6/7\n",
      "    RMSE (log): 0.19174\n",
      "\n",
      "  Fold 7/7\n",
      "    RMSE (log): 0.19488\n",
      "\n",
      "  Overall OOF RMSE (log): 0.21189 Â± 0.01385\n",
      "  Overall OOF RMSE (THB): à¸¿14034.59\n",
      "\n",
      "============================================================\n",
      "[4.5] Tuning classification threshold by RMSE on full train\n",
      "============================================================\n",
      "  Best threshold by RMSE: 0.1000  (Train RMSE: 10100.63)\n",
      "  Previous threshold by ACC: 0.4700\n",
      "\n",
      "============================================================\n",
      "[5/5] Generating Final Predictions\n",
      "============================================================\n",
      "\n",
      "Prediction Statistics:\n",
      "  Threshold: 0.1000\n",
      "  Predicted spenders: 25,889 / 25,889 (100.00%)\n",
      "  Mean prediction: à¸¿11458.63\n",
      "  Median prediction: à¸¿1009.45\n",
      "  Max prediction: à¸¿282003.88\n",
      "  Total predicted revenue: à¸¿296,652,593.67\n",
      "\n",
      "============================================================\n",
      "âœ“ Pipeline Complete! Submission saved as 'submission_enhanced.csv'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "RANDOM_STATE = 42\n",
    "TARGET = 'spending_30d'\n",
    "N_FOLDS = 7  # Increased for better generalization\n",
    "\n",
    "# GPU Configuration (Set to your available GPUs)\n",
    "USE_MULTI_GPU = True  # Enable multi-GPU support\n",
    "GPU_IDS = [0, 1]  # Your 2 GPUs\n",
    "\n",
    "# ==========================================\n",
    "# ADVANCED FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering focusing on:\n",
    "    - Interaction features\n",
    "    - Behavioral patterns\n",
    "    - Statistical aggregations\n",
    "    - Temporal features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === Base Preprocessing ===\n",
    "    cat_cols = ['primary_game', 'platform', 'vip_status', 'segment']\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna(-1).astype(int)\n",
    "    \n",
    "    zero_cols = [\n",
    "        'is_premium_member', 'guild_membership', 'owns_limited_edition', \n",
    "        'tournament_participation', 'friend_count', 'social_interactions', \n",
    "        'daily_login_streak', 'historical_spending', 'prev_month_spending',\n",
    "        'total_transactions', 'avg_transaction_value', 'purchases_on_discount'\n",
    "    ]\n",
    "    for col in zero_cols:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    df['days_since_last_purchase'] = df['days_since_last_purchase'].fillna(9999)\n",
    "    \n",
    "    # Fill remaining numeric columns\n",
    "    filled_cols = cat_cols + zero_cols + ['days_since_last_purchase', 'id', 'player_id', TARGET]\n",
    "    num_cols = [c for c in df.columns if c not in filled_cols]\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # === 1. SPENDING BEHAVIOR FEATURES ===\n",
    "    # Historical spending patterns\n",
    "    df['has_spent_before'] = (df['historical_spending'] > 0).astype(int)\n",
    "    df['spending_momentum'] = df['prev_month_spending'] / (df['historical_spending'] + 1)\n",
    "    df['spending_acceleration'] = df['prev_month_spending'] - df['historical_spending'] / 12  # Monthly avg\n",
    "    \n",
    "    # Transaction patterns\n",
    "    df['transactions_per_dollar'] = df['total_transactions'] / (df['historical_spending'] + 1)\n",
    "    df['avg_transaction_growth'] = df['avg_transaction_value'] / (df['historical_spending'] / (df['total_transactions'] + 1) + 1)\n",
    "    \n",
    "    # Purchase recency score\n",
    "    df['purchase_recency_score'] = 1 / (df['days_since_last_purchase'] + 1)\n",
    "    df['is_recent_buyer'] = (df['days_since_last_purchase'] < 30).astype(int)\n",
    "    df['is_dormant'] = (df['days_since_last_purchase'] > 180).astype(int)\n",
    "    \n",
    "    # Discount behavior\n",
    "    df['discount_dependency'] = df['purchases_on_discount'] / (df['total_transactions'] + 1)\n",
    "    df['full_price_purchases'] = df['total_transactions'] - df['purchases_on_discount']\n",
    "    \n",
    "    # === 2. ENGAGEMENT FEATURES ===\n",
    "    # Social engagement\n",
    "    df['social_engagement_ratio'] = df['social_interactions'] / (df['friend_count'] + 1)\n",
    "    df['is_social_player'] = (df['friend_count'] > df['friend_count'].median()).astype(int)\n",
    "    \n",
    "    # Activity intensity\n",
    "    df['activity_score'] = (\n",
    "        df['daily_login_streak'] * 0.3 + \n",
    "        df['social_interactions'] * 0.3 + \n",
    "        df['tournament_participation'] * 0.4\n",
    "    )\n",
    "    \n",
    "    # Commitment indicators\n",
    "    df['commitment_score'] = (\n",
    "        df['is_premium_member'] * 3 + \n",
    "        df['guild_membership'] * 2 + \n",
    "        df['owns_limited_edition'] * 2 +\n",
    "        (df['daily_login_streak'] > 7).astype(int) * 1\n",
    "    )\n",
    "    \n",
    "    # === 3. VALUE FEATURES ===\n",
    "    # Lifetime value indicators\n",
    "    df['ltv_score'] = df['historical_spending'] / (df['days_since_last_purchase'] + 1)\n",
    "    df['monthly_value'] = df['historical_spending'] / 12\n",
    "    df['value_consistency'] = df['avg_transaction_value'] / (df['monthly_value'] + 1)\n",
    "    \n",
    "    # Spending capacity\n",
    "    df['spending_capacity'] = df['avg_transaction_value'] * df['daily_login_streak']\n",
    "    df['whale_indicator'] = (df['avg_transaction_value'] > df['avg_transaction_value'].quantile(0.9)).astype(int)\n",
    "    \n",
    "    # === 4. INTERACTION FEATURES ===\n",
    "    # VIP interactions\n",
    "    df['vip_spending_ratio'] = df['historical_spending'] * (df['vip_status'] + 1)\n",
    "    df['vip_activity'] = df['activity_score'] * (df['vip_status'] + 1)\n",
    "    \n",
    "    # Premium features interaction\n",
    "    df['premium_features'] = (\n",
    "        df['is_premium_member'] + \n",
    "        df['guild_membership'] + \n",
    "        df['owns_limited_edition']\n",
    "    )\n",
    "    df['premium_spending'] = df['historical_spending'] * df['premium_features']\n",
    "    \n",
    "    # Segment interactions\n",
    "    df['segment_value'] = df['segment'] * df['avg_transaction_value']\n",
    "    df['segment_activity'] = df['segment'] * df['daily_login_streak']\n",
    "    \n",
    "    # === 5. STATISTICAL FEATURES ===\n",
    "    # Log transformations for skewed features\n",
    "    log_features = [\n",
    "        'historical_spending', 'prev_month_spending', 'total_transactions',\n",
    "        'avg_transaction_value', 'friend_count', 'social_interactions'\n",
    "    ]\n",
    "    for col in log_features:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_log1p'] = np.log1p(df[col])\n",
    "            df[f'{col}_sqrt'] = np.sqrt(df[col])\n",
    "    \n",
    "    # Binned features for categorical interactions\n",
    "    df['spending_tier'] = pd.qcut(df['historical_spending'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n",
    "    df['activity_tier'] = pd.qcut(df['daily_login_streak'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n",
    "    df['transaction_tier'] = pd.qcut(df['total_transactions'], q=5, labels=False, duplicates='drop').fillna(-1).astype(int)\n",
    "    \n",
    "    # === 6. RISK/CHURN FEATURES ===\n",
    "    df['churn_risk'] = (\n",
    "        (df['days_since_last_purchase'] > 90).astype(int) * 3 +\n",
    "        (df['daily_login_streak'] < 3).astype(int) * 2 +\n",
    "        (df['prev_month_spending'] == 0).astype(int) * 2\n",
    "    )\n",
    "    \n",
    "    # Engagement decay\n",
    "    df['engagement_decay'] = df['daily_login_streak'] / (df['days_since_last_purchase'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# OPTIMIZED MODEL PARAMETERS\n",
    "# ==========================================\n",
    "\n",
    "# Stage 1: Classification (Spender vs Non-Spender)\n",
    "PARAMS_S1_CAT = {\n",
    "    'iterations': 2000,\n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 8.0,\n",
    "    'border_count': 254,\n",
    "    'bagging_temperature': 0.8,\n",
    "    'random_strength': 1.5,\n",
    "    'scale_pos_weight': 1.0,  # Will be set dynamically\n",
    "    'eval_metric': 'Logloss',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': False,\n",
    "    'allow_writing_files': False,\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0:1' if USE_MULTI_GPU else '0'  # Use both GPUs\n",
    "}\n",
    "\n",
    "PARAMS_S1_LGB = {\n",
    "    'n_estimators': 1500,\n",
    "    'learning_rate': 0.02,\n",
    "    'num_leaves': 40,\n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 25,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_lambda': 3.0,\n",
    "    'reg_alpha': 1.5,\n",
    "    'scale_pos_weight': 1.0,  # Will be set dynamically\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbosity': -1,\n",
    "    'device': 'gpu',  # GPU acceleration\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0\n",
    "}\n",
    "\n",
    "# Stage 2: Regression (Amount Prediction)\n",
    "PARAMS_S2_CAT = {\n",
    "    'iterations': 5000,\n",
    "    'learning_rate': 0.008,\n",
    "    'depth': 7,\n",
    "    'l2_leaf_reg': 3.0,\n",
    "    'border_count': 200,\n",
    "    'bagging_temperature': 0.6,\n",
    "    'random_strength': 1.2,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': False,\n",
    "    'allow_writing_files': False,\n",
    "    'task_type': 'GPU',  # GPU acceleration\n",
    "    'devices': '0'\n",
    "}\n",
    "\n",
    "PARAMS_S2_LGB_MAIN = {\n",
    "    'n_estimators': 4500,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 64,\n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 15,\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'reg_lambda': 2.0,\n",
    "    'reg_alpha': 0.8,\n",
    "    'min_split_gain': 0.01,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbosity': -1,\n",
    "    'device': 'gpu',  # GPU acceleration\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0\n",
    "}\n",
    "\n",
    "PARAMS_S2_LGB_DEEP = {\n",
    "    'n_estimators': 4000,\n",
    "    'learning_rate': 0.006,\n",
    "    'num_leaves': 150,\n",
    "    'max_depth': 15,\n",
    "    'min_child_samples': 8,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_lambda': 4.0,\n",
    "    'reg_alpha': 0.5,\n",
    "    'min_split_gain': 0.005,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbosity': -1,\n",
    "    'device': 'gpu',  # GPU acceleration\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0\n",
    "}\n",
    "\n",
    "# Ensemble weights (tuned based on typical performance)\n",
    "W_S1 = {'cat': 0.55, 'lgb': 0.45}\n",
    "W_S2 = {'cat': 0.50, 'lgb_main': 0.30, 'lgb_deep': 0.20}\n",
    "\n",
    "# ==========================================\n",
    "# DATA LOADING & PREPROCESSING\n",
    "# ==========================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ADVANCED TWO-STAGE PREDICTION PIPELINE (GPU ACCELERATED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"\\nðŸ” Checking GPU availability...\")\n",
    "try:\n",
    "    import subprocess\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader']).decode()\n",
    "    print(f\"âœ“ GPU Detected: {gpu_info.strip()}\")\n",
    "except:\n",
    "    print(\"âš  GPU not detected. Models will fallback to CPU if GPU unavailable.\")\n",
    "\n",
    "print(\"\\n[1/5] Loading data...\")\n",
    "train = pd.read_csv('/kaggle/input/cpe342-karena/public_dataset/task3/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/cpe342-karena/public_dataset/task3/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
    "\n",
    "print(\"\\n[2/5] Creating advanced features...\")\n",
    "train_fe = create_advanced_features(train)\n",
    "test_fe = create_advanced_features(test)\n",
    "\n",
    "# Feature selection\n",
    "exclude_cols = ['id', 'player_id', TARGET, 'days_since_last_purchase']  # days_since excluded as it's often leaky\n",
    "feature_cols = [c for c in train_fe.columns if c not in exclude_cols]\n",
    "\n",
    "# Identify categorical features\n",
    "cat_features = [\n",
    "    'primary_game', 'platform', 'vip_status', 'segment',\n",
    "    'spending_tier', 'activity_tier', 'transaction_tier'\n",
    "]\n",
    "cat_features = [c for c in cat_features if c in feature_cols]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"Categorical features: {len(cat_features)}\")\n",
    "\n",
    "X = train_fe[feature_cols].copy()\n",
    "y_binary = (train_fe[TARGET] > 0).astype(int)\n",
    "X_test = test_fe[feature_cols].copy()\n",
    "\n",
    "# Convert categoricals\n",
    "for c in cat_features:\n",
    "    X[c] = X[c].astype('category')\n",
    "    X_test[c] = X_test[c].astype('category')\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 1: CLASSIFICATION\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"[3/5] STAGE 1: Spender Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate class imbalance\n",
    "pos_rate = y_binary.mean()\n",
    "scale_pos_weight = (1 - pos_rate) / pos_rate\n",
    "print(f\"Positive class rate: {pos_rate:.4f}\")\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "PARAMS_S1_CAT['scale_pos_weight'] = scale_pos_weight\n",
    "PARAMS_S1_LGB['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_prob_s1 = np.zeros(len(X))\n",
    "test_prob_s1 = np.zeros(len(X_test))\n",
    "fold_aucs = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X, y_binary), 1):\n",
    "    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n",
    "    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y_binary.iloc[tr_idx], y_binary.iloc[val_idx]\n",
    "    \n",
    "    # CatBoost\n",
    "    m1 = CatBoostClassifier(**PARAMS_S1_CAT, cat_features=cat_features)\n",
    "    m1.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=100)\n",
    "    p1_val = m1.predict_proba(X_val)[:, 1]\n",
    "    p1_test = m1.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # LightGBM\n",
    "    m2 = lgb.LGBMClassifier(**PARAMS_S1_LGB)\n",
    "    m2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    p2_val = m2.predict_proba(X_val)[:, 1]\n",
    "    p2_test = m2.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Ensemble\n",
    "    fold_pred = W_S1['cat'] * p1_val + W_S1['lgb'] * p2_val\n",
    "    oof_prob_s1[val_idx] = fold_pred\n",
    "    test_prob_s1 += (W_S1['cat'] * p1_test + W_S1['lgb'] * p2_test) / N_FOLDS\n",
    "    \n",
    "    fold_auc = roc_auc_score(y_val, fold_pred)\n",
    "    fold_aucs.append(fold_auc)\n",
    "    print(f\"    AUC: {fold_auc:.5f}\")\n",
    "\n",
    "# Overall metrics\n",
    "overall_auc = roc_auc_score(y_binary, oof_prob_s1)\n",
    "print(f\"\\n  Overall OOF AUC: {overall_auc:.5f} Â± {np.std(fold_aucs):.5f}\")\n",
    "\n",
    "# Optimize threshold (by accuracy, for reference only)\n",
    "best_acc = 0\n",
    "best_thresh_acc = 0.5\n",
    "for t in np.arange(0.2, 0.8, 0.005):\n",
    "    acc = accuracy_score(y_binary, (oof_prob_s1 > t).astype(int))\n",
    "    if acc > best_acc:\n",
    "        best_acc, best_thresh_acc = acc, t\n",
    "\n",
    "print(f\"  Optimal Threshold (by ACC): {best_thresh_acc:.4f} (Accuracy: {best_acc:.4f})\")\n",
    "\n",
    "# à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² default à¹€à¸œà¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¹„à¸”à¹‰ tune RMSE à¸ à¸²à¸¢à¸«à¸¥à¸±à¸‡\n",
    "best_thresh = best_thresh_acc\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 2: REGRESSION\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"[4/5] STAGE 2: Amount Prediction (Spenders Only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mask_spenders = train_fe[TARGET] > 0\n",
    "X_reg = train_fe[mask_spenders][feature_cols].reset_index(drop=True)\n",
    "y_reg = train_fe[mask_spenders][TARGET].reset_index(drop=True)\n",
    "y_reg_log = np.log1p(y_reg)\n",
    "\n",
    "print(f\"Training on {len(X_reg)} spenders\")\n",
    "print(f\"Mean spending: à¸¿{y_reg.mean():.2f}, Median: à¸¿{y_reg.median():.2f}\")\n",
    "\n",
    "# Convert categoricals\n",
    "for c in cat_features:\n",
    "    X_reg[c] = X_reg[c].astype('category')\n",
    "\n",
    "oof_amount_s2 = np.zeros(len(X_reg))\n",
    "test_amount_s2 = np.zeros(len(X_test))\n",
    "kf_reg = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "fold_rmses = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf_reg.split(X_reg), 1):\n",
    "    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n",
    "    X_tr, X_val = X_reg.iloc[tr_idx], X_reg.iloc[val_idx]\n",
    "    y_tr, y_val = y_reg_log.iloc[tr_idx], y_reg_log.iloc[val_idx]\n",
    "    \n",
    "    # CatBoost\n",
    "    r1 = CatBoostRegressor(**PARAMS_S2_CAT, cat_features=cat_features)\n",
    "    r1.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=150)\n",
    "    p1_val = r1.predict(X_val)\n",
    "    p1_test = r1.predict(X_test)\n",
    "    \n",
    "    # LightGBM Main\n",
    "    r2 = lgb.LGBMRegressor(**PARAMS_S2_LGB_MAIN)\n",
    "    r2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(150, verbose=False)])\n",
    "    p2_val = r2.predict(X_val)\n",
    "    p2_test = r2.predict(X_test)\n",
    "    \n",
    "    # LightGBM Deep\n",
    "    r3 = lgb.LGBMRegressor(**PARAMS_S2_LGB_DEEP)\n",
    "    r3.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(150, verbose=False)])\n",
    "    p3_val = r3.predict(X_val)\n",
    "    p3_test = r3.predict(X_test)\n",
    "    \n",
    "    # Ensemble (log space)\n",
    "    fold_pred = (\n",
    "        W_S2['cat'] * p1_val + \n",
    "        W_S2['lgb_main'] * p2_val + \n",
    "        W_S2['lgb_deep'] * p3_val\n",
    "    )\n",
    "    oof_amount_s2[val_idx] = fold_pred\n",
    "    test_amount_s2 += (\n",
    "        W_S2['cat'] * p1_test + \n",
    "        W_S2['lgb_main'] * p2_test + \n",
    "        W_S2['lgb_deep'] * p3_test\n",
    "    ) / N_FOLDS\n",
    "    \n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val, fold_pred))\n",
    "    fold_rmses.append(fold_rmse)\n",
    "    print(f\"    RMSE (log): {fold_rmse:.5f}\")\n",
    "\n",
    "# Evaluation\n",
    "rmse_log = np.sqrt(mean_squared_error(y_reg_log, oof_amount_s2))\n",
    "oof_amount_raw = np.expm1(oof_amount_s2)\n",
    "rmse_raw = np.sqrt(mean_squared_error(y_reg, oof_amount_raw))\n",
    "\n",
    "print(f\"\\n  Overall OOF RMSE (log): {rmse_log:.5f} Â± {np.std(fold_rmses):.5f}\")\n",
    "print(f\"  Overall OOF RMSE (THB): à¸¿{rmse_raw:.2f}\")\n",
    "# ==========================================\n",
    "# [4.5] Tuning threshold by RMSE on spending_30d\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"[4.5] Tuning classification threshold by RMSE on full train\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# y_true = à¸„à¹ˆà¸² spending à¸ˆà¸£à¸´à¸‡à¸—à¸±à¹‰à¸‡ train\n",
    "y_true = train_fe[TARGET].values  # shape (104000,)\n",
    "\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡ amount prediction à¹€à¸•à¹‡à¸¡à¸—à¸±à¹‰à¸‡ train (à¹ƒà¸™à¸«à¸™à¹ˆà¸§à¸¢à¹€à¸‡à¸´à¸™à¸ˆà¸£à¸´à¸‡)\n",
    "amount_raw_full = np.zeros_like(y_true, dtype=float)\n",
    "amount_raw_full[mask_spenders.values] = np.expm1(oof_amount_s2)\n",
    "\n",
    "best_t = None\n",
    "best_rmse = 1e18\n",
    "\n",
    "for t in np.linspace(0.10, 0.90, 81):  # 0.10, 0.11, ..., 0.90\n",
    "    y_pred = np.where(oof_prob_s1 > t, amount_raw_full, 0.0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_t = t\n",
    "\n",
    "print(f\"  Best threshold by RMSE: {best_t:.4f}  (Train RMSE: {best_rmse:.2f})\")\n",
    "print(f\"  Previous threshold by ACC: {best_thresh_acc:.4f}\")\n",
    "\n",
    "# à¹ƒà¸Šà¹‰ threshold à¹à¸šà¸š RMSE à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¸ªà¸³à¸«à¸£à¸±à¸š test\n",
    "best_thresh = best_t\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# FINAL PREDICTIONS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"[5/5] Generating Final Predictions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert test predictions from log to raw\n",
    "test_amount_raw = np.expm1(test_amount_s2)\n",
    "\n",
    "# Combine stages with threshold\n",
    "final_preds = np.where(test_prob_s1 > best_thresh, test_amount_raw, 0)\n",
    "\n",
    "# Safety clip\n",
    "final_preds = np.clip(final_preds, 0, 500000)\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"  Threshold: {best_thresh:.4f}\")\n",
    "print(f\"  Predicted spenders: {np.sum(final_preds > 0):,} / {len(final_preds):,} ({100*np.sum(final_preds > 0)/len(final_preds):.2f}%)\")\n",
    "print(f\"  Mean prediction: à¸¿{final_preds[final_preds > 0].mean():.2f}\")\n",
    "print(f\"  Median prediction: à¸¿{np.median(final_preds[final_preds > 0]):.2f}\")\n",
    "print(f\"  Max prediction: à¸¿{final_preds.max():.2f}\")\n",
    "print(f\"  Total predicted revenue: à¸¿{final_preds.sum():,.2f}\")\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'spending_30d': final_preds\n",
    "})\n",
    "submission.to_csv('submission_enhanced.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ Pipeline Complete! Submission saved as 'submission_enhanced.csv'\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14583263,
     "sourceId": 120121,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
